{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    ''' Dataset for loading and preprocessing the dataset '''\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 mode='train'\n",
    "              ):\n",
    "        self.mode = mode\n",
    "\n",
    "        # Read data into numpy arrays\n",
    "        with open(path, 'r') as fp:\n",
    "          data = list(csv.reader(fp))\n",
    "          data = np.array(data[1:])[:, 4:].astype(float)\n",
    "        \n",
    "        target = data[:, -1]\n",
    "        data = data[:, list(range(22))]\n",
    "\n",
    "        if mode == 'test':\n",
    "            # Testing data\n",
    "            self.data = torch.FloatTensor(data)\n",
    "            self.target = torch.LongTensor(target)\n",
    "        else:\n",
    "            # Training data (train/dev sets)\n",
    "        \n",
    "            # Splitting training data into train & dev sets\n",
    "            gap = int(len(data) * 0.8)\n",
    "            if mode == 'train':\n",
    "                \n",
    "                self.data = torch.FloatTensor(data[:gap])\n",
    "                self.target = torch.LongTensor(target[:gap])\n",
    "            elif mode == 'dev':\n",
    "                            \n",
    "            # Convert data into PyTorch tensors\n",
    "                self.data = torch.FloatTensor(data[gap:])\n",
    "                self.target = torch.LongTensor(target[gap:])\n",
    "\n",
    "        # Normalize features\n",
    "        self.data = \\\n",
    "            (self.data - self.data.mean(dim=0, keepdim=True)) \\\n",
    "            / self.data.std(dim=0, keepdim=True)\n",
    "\n",
    "        self.dim = self.data.shape[1]\n",
    "\n",
    "        # print(f'Finished reading the {mode} set of Dataset ({len(self.data)} samples found, each dim = {self.dim})')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.data[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(22, 512)\n",
    "        self.layer1_bn=nn.BatchNorm1d(512)\n",
    "\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer2_bn=nn.BatchNorm1d(256)\n",
    "\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer3_bn=nn.BatchNorm1d(128)\n",
    "\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer4_bn=nn.BatchNorm1d(64)\n",
    "\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer5_bn=nn.BatchNorm1d(32)\n",
    "\n",
    "        # self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.out = nn.Linear(32, 3) \n",
    "\n",
    "        # self.act_fn = nn.Sigmoid()\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer1_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer2_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer3_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    " \n",
    "        x = self.layer4(x)\n",
    "        x = self.layer4_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "plt.ioff()\n",
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "same_seeds(0)\n",
    "\n",
    "# get device \n",
    "device = 'cpu'\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# training parameters\n",
    "num_epoch = 400        # number of training epoch\n",
    "\n",
    "learning_rate = 0.001       # learning rate\n",
    "\n",
    "\n",
    "def macro_precision(y_true, y_predict):\n",
    "    hit_count = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "    for i, prediction in enumerate(y_predict):\n",
    "        if y_true[i] == prediction:\n",
    "            hit_count[prediction] += 1\n",
    "    \n",
    "    # r1 = hit_count[0]/y_true.count(0)\n",
    "    # r2 = hit_count[1]/y_true.count(1)\n",
    "    # r3 = hit_count[2]/y_true.count(2)\n",
    "\n",
    "    p1 = hit_count[0]/y_predict.count(0)\n",
    "    p2 = hit_count[1]/y_predict.count(1)\n",
    "    p3 = hit_count[2]/y_predict.count(2)\n",
    "\n",
    "    # f1 = 2 * p1 * r1 / (p1 + r1)\n",
    "    # f2 = 2 * p2 * r2 / (p2 + r2)\n",
    "    # f3 = 2 * p3 * r3 / (p3 + r3)\n",
    "    \n",
    "    return (p1 + p2 + p3) / 3\n",
    "\n",
    "def plot_learning_curve(acc_record, title, y_label, i , d):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_epoch = len(acc_record['train'])\n",
    "    x = range(total_epoch)\n",
    "    # x_2 = x_1[::len(acc_record['train']) // len(acc_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x, acc_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x, acc_record['dev'], c='tab:cyan', label='dev')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./models/fig/{y_label}_{i}_{d}d.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 2\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 3\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 4\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 5\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 6\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 7\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 8\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 9\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 10\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 11\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 12\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 13\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 14\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 15\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 16\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 17\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 18\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 19\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 20\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 21\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 22\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 23\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 24\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 25\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 26\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 27\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 28\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 29\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 30\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 31\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 32\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 33\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 34\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 35\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 36\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 37\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 38\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 39\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 40\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 41\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 42\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 43\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 44\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 45\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 46\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 47\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 48\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 49\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 50\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 51\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 52\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 53\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 54\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 55\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 56\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 57\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 58\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 59\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 60\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 61\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 62\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 63\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 64\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 65\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 66\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 67\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 68\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 69\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 70\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 71\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 72\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 73\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 74\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 75\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 76\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 77\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 78\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 79\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 80\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 81\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 82\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 83\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 84\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 85\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 86\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 87\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 88\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 89\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 90\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 91\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 92\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 93\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 94\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 95\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 96\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 97\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 98\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 99\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 100\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 101\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 102\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 103\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 104\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 105\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 106\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 107\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 108\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 109\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 110\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 111\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 112\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 113\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 114\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 115\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 116\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 117\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 118\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 119\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 120\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 121\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 122\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 123\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 124\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 125\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 126\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 127\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 128\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 129\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 130\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 131\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 132\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 133\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 134\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 135\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 136\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 137\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 138\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 139\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 140\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 141\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 142\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 143\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 144\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 145\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 146\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 147\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 148\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 149\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 150\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 151\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 152\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 153\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 154\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 155\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 156\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 157\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 158\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 159\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 160\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 161\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 162\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 163\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 164\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 165\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 166\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 167\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 168\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 169\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 170\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 171\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 172\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 173\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 174\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 175\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 176\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 177\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 178\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 179\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 180\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 181\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 182\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 183\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 184\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 185\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 186\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 187\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 188\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 189\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 190\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 191\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 192\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 193\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 194\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 195\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 196\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 197\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 198\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 199\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n",
      "Model 200\n",
      "15d\n",
      "30d\n",
      "90d\n",
      "180d\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score  \n",
    "\n",
    "error_list = []\n",
    "for i in range(1, 201):\n",
    "    print(f'Model {i}')\n",
    "    for d in [15, 30, 90, 180]:\n",
    "        print(f'{d}d')\n",
    "        try:\n",
    "            train_set = StockDataset(f'./data/training_data/training_data_{i}_{d}d.csv', mode = 'test')\n",
    "            val_set = StockDataset(f'./data/training_data/testing_data_{i}_{d}d.csv', mode = 'test')\n",
    "            train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
    "            val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            # the path where checkpoint saved\n",
    "            model_path = f'./models/models/model_{i}_{d}d.ckpt'\n",
    "\n",
    "            # create model, define a loss function, and optimizer\n",
    "            model = Classifier().to(device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Training\n",
    "\n",
    "            best_acc = 0.0\n",
    "            best_macro_precision = 0.0\n",
    "\n",
    "            acc_record = {'train': [], 'dev': []}\n",
    "            macro_f1_record = {'train': [], 'dev': []}\n",
    "            macro_precision_score_record = {'train': [], 'dev': []}\n",
    "\n",
    "            # init accuracy\n",
    "            train_acc = 0.0\n",
    "            val_acc = 0.0\n",
    "\n",
    "            train_labels = []\n",
    "            train_predictions = []\n",
    "            val_labels = []\n",
    "            val_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, val_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() \n",
    "                    for y in val_pred.cpu().numpy():\n",
    "                        val_predictions.append(y)\n",
    "\n",
    "                    for y in labels.cpu().numpy():\n",
    "                        val_labels.append(y)\n",
    "                \n",
    "                for data in train_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs) \n",
    "                    _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                    train_acc += (train_pred.cpu() == labels.cpu()).sum().item() \n",
    "                    for y in train_pred.cpu().numpy():\n",
    "                        train_predictions.append(y)\n",
    "\n",
    "                    for y in labels.cpu().numpy():\n",
    "                        train_labels.append(y)\n",
    "\n",
    "                acc_record['dev'].append(val_acc/len(val_set))\n",
    "                acc_record['train'].append(train_acc/len(train_set))\n",
    "                macro_f1_record['dev'].append(f1_score(val_labels, val_predictions, average='macro'))\n",
    "                macro_f1_record['train'].append(f1_score(train_labels, train_predictions, average='macro'))\n",
    "                macro_precision_score_record['dev'].append(macro_precision(val_labels, val_predictions))\n",
    "                macro_precision_score_record['train'].append(macro_precision(train_labels, train_predictions))\n",
    "\n",
    "            # start training\n",
    "            for epoch in range(num_epoch):\n",
    "            \n",
    "                train_acc = 0.0\n",
    "                train_loss = 0.0\n",
    "                val_acc = 0.0\n",
    "                val_loss = 0.0\n",
    "\n",
    "                train_labels = []\n",
    "                train_predictions = []\n",
    "                val_labels = []\n",
    "                val_predictions = []\n",
    "\n",
    "                # training\n",
    "                model.train() # set the model to training mode\n",
    "                for data in train_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad() \n",
    "                    outputs = model(inputs) \n",
    "                    batch_loss = criterion(outputs, labels)\n",
    "                    _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                    batch_loss.backward() # compute gradient\n",
    "                    optimizer.step() # update model with optimizer\n",
    "\n",
    "                    train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "                    train_loss += batch_loss.item()\n",
    "                    for y in train_pred.cpu().numpy():\n",
    "                        train_predictions.append(y)\n",
    "\n",
    "                    for y in labels.cpu().numpy():\n",
    "                        train_labels.append(y)\n",
    "\n",
    "                \n",
    "                acc_record['train'].append(train_acc/len(train_set))\n",
    "\n",
    "                train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "                macro_f1_record['train'].append(train_f1)\n",
    "\n",
    "                train_weighted_precision = macro_precision(train_labels, train_predictions)\n",
    "                macro_precision_score_record['train'].append(train_weighted_precision)\n",
    "\n",
    "                # validation\n",
    "\n",
    "                model.eval() # set the model to evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    for data in val_loader:\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        batch_loss = criterion(outputs, labels) \n",
    "                        _, val_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                    \n",
    "                        val_acc += (val_pred.cpu() == labels.cpu()).sum().item() \n",
    "                        val_loss += batch_loss.item()\n",
    "                        for y in val_pred.cpu().numpy():\n",
    "                            val_predictions.append(y)\n",
    "\n",
    "                        for y in labels.cpu().numpy():\n",
    "                            val_labels.append(y)\n",
    "\n",
    "\n",
    "                    acc_record['dev'].append(val_acc/len(val_set))\n",
    "\n",
    "                    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "                    macro_f1_record['dev'].append(val_f1)\n",
    "\n",
    "                    val_weighted_precision = macro_precision(val_labels, val_predictions)\n",
    "                    macro_precision_score_record['dev'].append(val_weighted_precision)\n",
    "\n",
    "\n",
    "                    # if the model improves, save a checkpoint at this epoch\n",
    "                    if val_weighted_precision > best_macro_precision:\n",
    "                        best_macro_precision = val_weighted_precision\n",
    "                        torch.save(model.state_dict(), model_path)\n",
    "                        # print('[{:03d}/{:03d}] Train Acc: {:3.6f} F1: {:.3f} wP: {:.3f} Loss: {:3.6f} | \\\n",
    "                        # Val Acc: {:3.6f} F1: {:.3f} wP: {:.3f} loss: {:3.6f}'\n",
    "                            # .format(epoch + 1, num_epoch, train_acc/len(train_set), train_f1, train_weighted_precision, train_loss/len(train_loader), \n",
    "                            #                             val_acc/len(val_set), val_f1, val_weighted_precision, val_loss/len(val_loader)))\n",
    "                        # print('saving model with Val Acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "                        # print('saving model with Val Precision {:.3f}'.format(best_macro_precision))\n",
    "                        early_stop_cnt = 0\n",
    "                    else:\n",
    "                        early_stop_cnt += 1\n",
    "\n",
    "            plot_learning_curve(acc_record, 'deep model', 'Accuracy', i, d)\n",
    "            plot_learning_curve(macro_precision_score_record, 'deep model', 'Macro Precision', i, d)\n",
    "            plot_learning_curve(macro_f1_record, 'deep model', 'Macro F1 Score', i, d)\n",
    "        except Exception as e:\n",
    "            error_list.append([i, d, str(e)])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3,\n",
       "  30,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [4,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [8,\n",
       "  90,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [11,\n",
       "  15,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [13,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [16, 180, 'division by zero'],\n",
       " [22,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [27,\n",
       "  90,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [31,\n",
       "  90,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [46,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [49, 15, 'division by zero'],\n",
       " [69, 30, 'division by zero'],\n",
       " [70,\n",
       "  90,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [70, 180, 'division by zero'],\n",
       " [72,\n",
       "  30,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [84,\n",
       "  15,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [94,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [103,\n",
       "  15,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [104, 180, 'division by zero'],\n",
       " [109,\n",
       "  15,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [110,\n",
       "  30,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [118,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [120,\n",
       "  30,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [121, 180, 'division by zero'],\n",
       " [122,\n",
       "  30,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [131,\n",
       "  15,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [132, 15, 'division by zero'],\n",
       " [136,\n",
       "  30,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [140,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [141,\n",
       "  15,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [141,\n",
       "  90,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [145,\n",
       "  90,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [167,\n",
       "  90,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [193,\n",
       "  15,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [194,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])'],\n",
       " [199,\n",
       "  180,\n",
       "  'Expected more than 1 value per channel when training, got input size torch.Size([1, 512])']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bad007a486731be1b72cebf8069ca022872976a844f0a9d919fbbe79314a6909"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('thesis-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
