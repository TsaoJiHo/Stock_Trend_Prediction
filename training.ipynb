{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入成功!\n"
     ]
    }
   ],
   "source": [
    "import finlab\n",
    "from finlab import data\n",
    "# https://ai.finlab.tw/api_token/\n",
    "finlab.login('GSCV5+L51GA2wGPvFZYpqKai2rKE9Tw2eG1FVGPX+hzn2SF1oFvvSj1N7fCJLTrB#free')\n",
    "\n",
    "data.get('price:收盤價').to_csv('./data/indicator_data/close.csv')\n",
    "data.get('taiex_total_index:收盤指數').to_csv('./data/indicator_data/taiex.csv')\n",
    "\n",
    "data.indicator('RSI', timeperiod=14).to_csv('./data/indicator_data/rsi_14.csv')\n",
    "data.indicator('RSI', timeperiod=42).to_csv('./data/indicator_data/rsi_42.csv')\n",
    "\n",
    "\n",
    "k_9, d_9 = data.indicator('STOCH', timeperiod=9)\n",
    "k_9.to_csv('./data/indicator_data/k_9.csv')\n",
    "d_9.to_csv('./data/indicator_data/d_9.csv')\n",
    "\n",
    "k_27, d_27 = data.indicator('STOCH', timeperiod=27)\n",
    "k_27.to_csv('./data/indicator_data/k_27.csv')\n",
    "d_27.to_csv('./data/indicator_data/d_27.csv')\n",
    "\n",
    "macd = data.indicator('MACD')\n",
    "macd[0].to_csv('./data/indicator_data/dif.csv')\n",
    "macd[1].to_csv('./data/indicator_data/macd.csv')\n",
    "\n",
    "data.indicator('WILLR', timeperiod=14).to_csv('./data/indicator_data/willr_14.csv')\n",
    "data.indicator('WILLR', timeperiod=42).to_csv('./data/indicator_data/willr_42.csv')\n",
    "\n",
    "data.indicator('SMA', timeperiod=20).to_csv('./data/indicator_data/sma_20.csv')\n",
    "data.indicator('SMA', timeperiod=60).to_csv('./data/indicator_data/sma_60.csv')\n",
    "\n",
    "data.indicator('EMA', timeperiod=20).to_csv('./data/indicator_data/ema_20.csv')\n",
    "data.indicator('EMA', timeperiod=60).to_csv('./data/indicator_data/ema_60.csv')\n",
    "\n",
    "data.indicator('ATR', timeperiod=14).to_csv('./data/indicator_data/atr_14.csv')\n",
    "data.indicator('ATR', timeperiod=42).to_csv('./data/indicator_data/atr_42.csv')\n",
    "\n",
    "data.indicator('ADX', timeperiod=14).to_csv('./data/indicator_data/adx_14.csv')\n",
    "data.indicator('ADX', timeperiod=42).to_csv('./data/indicator_data/adx_42.csv')\n",
    "\n",
    "data.indicator('CCI', timeperiod=14).to_csv('./data/indicator_data/cci_14.csv')\n",
    "data.indicator('CCI', timeperiod=42).to_csv('./data/indicator_data/cci_42.csv')\n",
    "\n",
    "data.indicator('ROC', timeperiod=10).to_csv('./data/indicator_data/roc_10.csv')\n",
    "data.indicator('ROC', timeperiod=30).to_csv('./data/indicator_data/roc_30.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Stock Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.concat([pd.read_csv('./data/StockList_1.csv'), pd.read_csv('./data/StockList_2.csv')])\n",
    "df = df[df['市場']=='市']\n",
    "code_serie = df['代號']\n",
    "stock_serie = df['名稱']\n",
    "close_df = pd.read_csv('./data/indicator_data/close.csv')\n",
    "with open('codes_300.txt', 'w') as f1, open('stocks_300.txt', 'w') as f2:\n",
    "    for i, code in enumerate(code_serie):\n",
    "        stock = stock_serie.iloc[i]\n",
    "        code = code.replace('\\\"', '').replace('=', '')\n",
    "        if len(close_df[code].dropna()) >= 2530:\n",
    "            f1.write(code + ' ')\n",
    "            f2.write(stock + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Label Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def triple_barrier(price, ub, lb, max_period):\n",
    "\n",
    "    def end_price(s):\n",
    "        return np.append(s[(s / s[0] > ub) | (s / s[0] < lb)], s[-1])[0]/s[0]\n",
    "    \n",
    "    r = np.array(range(max_period))\n",
    "    \n",
    "    def end_time(s):\n",
    "        return np.append(r[(s / s[0] > ub) | (s / s[0] < lb)], max_period-1)[0]\n",
    "\n",
    "    p = price.rolling(max_period).apply(end_price, raw=True).shift(-max_period+1)\n",
    "    t = price.rolling(max_period).apply(end_time, raw=True).shift(-max_period+1)\n",
    "    t = pd.Series([t.index[int(k+i)] if not math.isnan(k+i) else np.datetime64('NaT') \n",
    "                   for i, k in enumerate(t)], index=t.index).dropna()\n",
    "\n",
    "    signal = pd.Series(1, p.index)\n",
    "    signal.loc[p > ub] = 2\n",
    "    signal.loc[p < lb] = 0\n",
    "    signal[-(max_period-1):] = np.nan\n",
    "    ret = pd.DataFrame({'triple_barrier_profit':p, 'triple_barrier_sell_time':t, 'triple_barrier_signal':signal})\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "close_df = pd.read_csv('./data/indicator_data/close.csv')\n",
    "\n",
    "with open('codes_300.txt', 'r') as f:\n",
    "    codes = f.read().split()\n",
    "\n",
    "code_para_map = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for code in tqdm(codes): \n",
    "    close_serie = close_df[code].rename('close')\n",
    "\n",
    "    for d in [10, 20, 30, 40, 60, 120]:\n",
    "        up_threshold = 1.01\n",
    "        down_threshold = 0.99\n",
    "        label_name = str(d)+'d'\n",
    "        label_serie = triple_barrier(close_serie, up_threshold, down_threshold, d + 1)['triple_barrier_signal'].rename(label_name)\n",
    "\n",
    "        df = pd.concat([close_serie, label_serie], axis=1)\n",
    "        df = df.dropna()\n",
    "        df = df[83:]\n",
    "\n",
    "        while len(df[label_name].value_counts()) < 3 or df[label_name].value_counts()[2] > df[label_name].value_counts()[1]:\n",
    "            up_threshold += 0.001\n",
    "            down_threshold -= 0.001\n",
    "            label_serie = triple_barrier(close_serie, up_threshold, down_threshold, d + 1)['triple_barrier_signal'].rename(label_name)\n",
    "\n",
    "            df = pd.concat([close_serie, label_serie], axis=1)\n",
    "\n",
    "            df = df.dropna()\n",
    "            df = df[83:]\n",
    "        code_para_map[code][label_name] += [up_threshold, down_threshold]\n",
    "\n",
    "for code in code_para_map:\n",
    "    for d in code_para_map[code]:\n",
    "        code_para_map[code][d][0] = round(code_para_map[code][d][0], 3)\n",
    "        code_para_map[code][d][1] = round(code_para_map[code][d][1], 3)\n",
    "    \n",
    "with open(\"trend_params_300.json\", \"w\") as outfile:\n",
    "    json.dump(code_para_map, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355/355 [03:23<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def data_generator(codes):\n",
    "\n",
    "    date_serie = pd.read_csv('./data/indicator_data/close.csv')['date']\n",
    "\n",
    "    close_df = pd.read_csv('./data/indicator_data/close.csv')\n",
    "    roc_10_df = pd.read_csv('./data/indicator_data/roc_10.csv')\n",
    "    roc_30_df = pd.read_csv('./data/indicator_data/roc_30.csv')\n",
    "    cci_14_df = pd.read_csv('./data/indicator_data/cci_14.csv')\n",
    "    cci_42_df = pd.read_csv('./data/indicator_data/cci_42.csv')\n",
    "    adx_14_df = pd.read_csv('./data/indicator_data/adx_14.csv')\n",
    "    adx_42_df = pd.read_csv('./data/indicator_data/adx_42.csv')\n",
    "    atr_14_df = pd.read_csv('./data/indicator_data/atr_14.csv')\n",
    "    atr_42_df = pd.read_csv('./data/indicator_data/atr_42.csv')\n",
    "    ema_20_df = pd.read_csv('./data/indicator_data/ema_20.csv')\n",
    "    ema_60_df = pd.read_csv('./data/indicator_data/ema_60.csv')\n",
    "    sma_20_df = pd.read_csv('./data/indicator_data/sma_20.csv')\n",
    "    sma_60_df = pd.read_csv('./data/indicator_data/sma_60.csv')\n",
    "    willr_14_df = pd.read_csv('./data/indicator_data/willr_14.csv')\n",
    "    willr_42_df = pd.read_csv('./data/indicator_data/willr_42.csv')\n",
    "    dif_df = pd.read_csv('./data/indicator_data/dif.csv')\n",
    "    macd_df = pd.read_csv('./data/indicator_data/macd.csv')\n",
    "    k_9_df = pd.read_csv('./data/indicator_data/k_9.csv')\n",
    "    d_9_df = pd.read_csv('./data/indicator_data/d_9.csv')\n",
    "    k_27_df = pd.read_csv('./data/indicator_data/k_27.csv')\n",
    "    d_27_df = pd.read_csv('./data/indicator_data/d_27.csv')\n",
    "    rsi_14_df = pd.read_csv('./data/indicator_data/rsi_14.csv')\n",
    "    rsi_42_df = pd.read_csv('./data/indicator_data/rsi_42.csv')\n",
    "\n",
    "\n",
    "    with open('trend_params_300.json') as jsonfile:\n",
    "        code_para_map = json.load(jsonfile)\n",
    "\n",
    "    for i, code in enumerate(tqdm(codes)):\n",
    "        \n",
    "        close_serie = close_df[code].rename('close')\n",
    "        series = [date_serie, close_serie]\n",
    "        series.append(roc_10_df[code].rename('roc_10'))\n",
    "        series.append(roc_30_df[code].rename('roc_30'))\n",
    "        series.append(cci_14_df[code].rename('cci_14'))\n",
    "        series.append(cci_42_df[code].rename('cci_42'))\n",
    "        series.append(adx_14_df[code].rename('adx_14'))\n",
    "        series.append(adx_42_df[code].rename('adx_42'))\n",
    "        series.append(atr_14_df[code].rename('atr_14'))\n",
    "        series.append(atr_42_df[code].rename('atr_42'))\n",
    "        series.append(ema_20_df[code].rename('ema_20'))\n",
    "        series.append(ema_60_df[code].rename('ema_60'))\n",
    "        series.append(sma_20_df[code].rename('sma_20'))\n",
    "        series.append(sma_60_df[code].rename('sma_60'))\n",
    "        series.append(willr_14_df[code].rename('willr_14'))\n",
    "        series.append(willr_42_df[code].rename('willr_42'))\n",
    "        series.append(macd_df[code].rename('macd'))\n",
    "        series.append(dif_df[code].rename('dif'))\n",
    "        series.append(k_9_df[code].rename('k_9'))\n",
    "        series.append(d_9_df[code].rename('d_9'))\n",
    "        series.append(k_27_df[code].rename('k_27'))\n",
    "        series.append(d_27_df[code].rename('d_27'))\n",
    "        series.append(rsi_14_df[code].rename('rsi_14'))\n",
    "        series.append(rsi_42_df[code].rename('rsi_42'))\n",
    "\n",
    "        params = code_para_map[code]\n",
    "\n",
    "        series.append(triple_barrier(close_serie, params['10d'][0], params['10d'][1], 11)['triple_barrier_signal'].rename('10d'))\n",
    "        series.append(triple_barrier(close_serie, params['20d'][0], params['20d'][1], 21)['triple_barrier_signal'].rename('20d'))\n",
    "        series.append(triple_barrier(close_serie, params['30d'][0], params['30d'][1], 31)['triple_barrier_signal'].rename('30d'))\n",
    "        series.append(triple_barrier(close_serie, params['40d'][0], params['40d'][1], 41)['triple_barrier_signal'].rename('40d'))\n",
    "        series.append(triple_barrier(close_serie, params['60d'][0], params['60d'][1], 61)['triple_barrier_signal'].rename('60d'))\n",
    "        series.append(triple_barrier(close_serie, params['120d'][0], params['120d'][1], 121)['triple_barrier_signal'].rename('120d'))\n",
    "        df = pd.concat(series, axis=1)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "        df.to_csv(f'./data/training_data_300/raw_data_{i+1}.csv')\n",
    "\n",
    "\n",
    "\n",
    "with open('codes_300.txt', 'r') as f:\n",
    "    codes = f.read().split()\n",
    "\n",
    "data_generator(codes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "355it [01:40,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "SPLIT_DATE = '2021-01-01'\n",
    "\n",
    "def _insert_row_(row_number, df):\n",
    "    # Slice the upper half of the dataframe\n",
    "    df1 = df[0:row_number]\n",
    "  \n",
    "    # Store the result of lower half of the dataframe\n",
    "    df2 = df[row_number:]\n",
    "  \n",
    "    # Insert the row in the upper half dataframe\n",
    "    # df1.loc[row_number]=df1.iloc[row_number-1]\n",
    "    df1 = df1.append(df1.iloc[row_number-1] , ignore_index =True)\n",
    "  \n",
    "    # Concat the two dataframes\n",
    "    df_result = pd.concat([df1, df2])\n",
    "  \n",
    "    # Reassign the index labels\n",
    "    df_result.index = [*range(df_result.shape[0])]\n",
    "  \n",
    "    # Return the updated dataframe\n",
    "    return df_result\n",
    "\n",
    "def preprocess(path, label, test_only = False):\n",
    "    label_loc_map = {'10d': 25, '20d': 26, '30d':27, '40d':28, '60d': 29, '120d': 30}\n",
    "    df = pd.read_csv(path) # raw data\n",
    "\n",
    "    # generate close df for stable fluctuation calculating\n",
    "    close_df = df[['date', 'close']]\n",
    "    close_df = close_df[close_df['date'] >= SPLIT_DATE]\n",
    "    close_df = close_df.dropna() # drop missing close\n",
    "\n",
    "    testing_date = pd.read_csv('./data/indicator_data/taiex.csv')\n",
    "    testing_date = testing_date[testing_date['date'] >= SPLIT_DATE]['date']\n",
    "\n",
    "    for j, date in enumerate(testing_date):\n",
    "        if date not in close_df['date'].values:\n",
    "            close_df = _insert_row_(j, close_df)\n",
    "    close_df.to_csv(pathlib.Path(path.parent/(f'close_df_{path.stem.split(\"_\")[-1]}')).with_suffix('.csv'))\n",
    "\n",
    "    df = df.iloc[:, list(range(25)) + [label_loc_map[label]]]\n",
    "    df = df.dropna() # drop front, rear and missing close\n",
    "\n",
    "    # split test data\n",
    "    testing_df = df[df['date'] >= SPLIT_DATE]\n",
    "    # testing_df = testing_df[testing_df['date'] <= '2011-03-02']\n",
    "    d = int(label[:-1])\n",
    "\n",
    "    testing_date = testing_date[:-d]\n",
    "\n",
    "    for j, date in enumerate(testing_date):\n",
    "        if date not in testing_df['date'].values:\n",
    "            testing_df = _insert_row_(j, testing_df)\n",
    "    testing_df.to_csv(pathlib.Path(path.parent/(f'testing_data_{path.stem.split(\"_\")[-1]}_{label}')).with_suffix('.csv'))\n",
    "\n",
    "    if test_only:\n",
    "        return\n",
    "\n",
    "    # oversampling training data\n",
    "    df = df[df['date'] < SPLIT_DATE]\n",
    "    value_counts_list = [df[label].value_counts()[0], df[label].value_counts()[1], df[label].value_counts()[2]]\n",
    "\n",
    "    down_dif = max(value_counts_list) - value_counts_list[0]\n",
    "    stable_dif = max(value_counts_list) - value_counts_list[1]\n",
    "    up_dif = max(value_counts_list) - value_counts_list[2]\n",
    "\n",
    "\n",
    "    resample_df = [df, df[df[label] == 0].sample(n = down_dif, replace=(value_counts_list[0] < down_dif))\n",
    "    , df[df[label] == 1].sample(n = stable_dif, replace=(value_counts_list[1] < stable_dif))\n",
    "    , df[df[label] == 2].sample(n = up_dif, replace=(value_counts_list[2] < up_dif))]\n",
    "\n",
    "\n",
    "    pd.concat(resample_df).to_csv(pathlib.Path(path.parent/(f'training_data_{path.stem.split(\"_\")[-1]}_{label}')).with_suffix('.csv'))\n",
    "\n",
    "\n",
    "error_list = []\n",
    "for path in tqdm(pathlib.Path('./data/training_data_300').glob('raw_data_*.csv')):\n",
    "    for label in ['10d', '20d', '30d', '40d', '60d', '120d']:\n",
    "\n",
    "        try:\n",
    "            preprocess(path, label, test_only=False)\n",
    "        except Exception as e:\n",
    "            error_list.append((path, label, e))\n",
    "            continue\n",
    "\n",
    "print(error_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    ''' Dataset for loading and preprocessing the dataset '''\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 mode='train'\n",
    "              ):\n",
    "        self.mode = mode\n",
    "\n",
    "        # Read data into numpy arrays\n",
    "        with open(path, 'r') as fp:\n",
    "            data = list(csv.reader(fp))\n",
    "            data = np.array(data[1:])[:, 4:].astype(float)\n",
    "        \n",
    "        \n",
    "\n",
    "        target = data[:, -1]\n",
    "        data = data[:, list(range(22))]\n",
    "\n",
    "        \n",
    "\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.target = torch.LongTensor(target)\n",
    "\n",
    "\n",
    "        # Normalize features\n",
    "        self.data = \\\n",
    "            (self.data - self.data.mean(dim=0, keepdim=True)) \\\n",
    "            / self.data.std(dim=0, keepdim=True)\n",
    "\n",
    "        self.dim = self.data.shape[1]\n",
    "\n",
    "        # print(f'Finished reading the {mode} set of Dataset ({len(self.data)} samples found, each dim = {self.dim})')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.data[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layer2 = nn.Linear(22, 256)\n",
    "        self.layer2_bn=nn.BatchNorm1d(256)\n",
    "\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer3_bn=nn.BatchNorm1d(128)\n",
    "\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer4_bn=nn.BatchNorm1d(64)\n",
    "\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer5_bn=nn.BatchNorm1d(32)\n",
    "\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.out = nn.Linear(32, 3) \n",
    "        \n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer2_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer3_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.drop(x)\n",
    " \n",
    "        x = self.layer4(x)\n",
    "        x = self.layer4_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score \n",
    "plt.ioff()\n",
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "same_seeds(0)\n",
    "\n",
    "# get device \n",
    "device = 'cpu'\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# training parameters\n",
    "num_epoch = 100        # number of training epoch\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.001       # learning rate\n",
    "\n",
    "\n",
    "def profit_count(y_true, y_predict):\n",
    "    hit_count = {0: 0, 1: 0, 2: 0}\n",
    "    loss_count = 0\n",
    "    for i, prediction in enumerate(y_predict):\n",
    "        if y_true[i] == prediction:\n",
    "            hit_count[prediction] += 1\n",
    "        elif (y_true[i] == 0 and prediction == 2) or (y_true[i] == 2 and prediction == 0):\n",
    "            loss_count += 1\n",
    "    # r1 = hit_count[0]/y_true.count(0)\n",
    "    # r2 = hit_count[1]/y_true.count(1)\n",
    "    # r3 = hit_count[2]/y_true.count(2)\n",
    "\n",
    "    if hit_count[0] == 0:\n",
    "        p1 = 0\n",
    "    else:\n",
    "        p1 = hit_count[0]/y_predict.count(0)\n",
    "    if hit_count[1] == 0:\n",
    "        p2 = 0\n",
    "    else:\n",
    "        p2 = hit_count[1]/y_predict.count(1)\n",
    "    if hit_count[2] == 0:\n",
    "        p3 = 0\n",
    "    else:\n",
    "        p3 = hit_count[2]/y_predict.count(2)\n",
    "\n",
    "    # f1 = 2 * p1 * r1 / (p1 + r1)\n",
    "    # f2 = 2 * p2 * r2 / (p2 + r2)\n",
    "    # f3 = 2 * p3 * r3 / (p3 + r3)\n",
    "    \n",
    "    # return (p1 + p2 + p3) / 3\n",
    "    return hit_count[0] + hit_count[2] - loss_count\n",
    "\n",
    "def plot_learning_curve(acc_record, title, y_label, i , d, limit_y = True):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_epoch = len(acc_record['train'])\n",
    "    x = range(total_epoch)\n",
    "    # x_2 = x_1[::len(acc_record['train']) // len(acc_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x, acc_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x, acc_record['dev'], c='tab:cyan', label='dev')\n",
    "    if limit_y:\n",
    "        plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./models_300/fig/{y_label}_{i}_{d}d.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Model 2\n",
      "Model 3\n",
      "Model 4\n",
      "Model 5\n",
      "Model 6\n",
      "Model 7\n",
      "Model 8\n",
      "Model 9\n",
      "Model 10\n",
      "Model 11\n",
      "Model 12\n",
      "Model 13\n",
      "Model 14\n",
      "Model 15\n",
      "Model 16\n",
      "Model 17\n",
      "Model 18\n",
      "Model 19\n",
      "Model 20\n",
      "Model 21\n",
      "Model 22\n",
      "Model 23\n",
      "Model 24\n",
      "Model 25\n",
      "Model 26\n",
      "Model 27\n",
      "Model 28\n",
      "Model 29\n",
      "Model 30\n",
      "Model 31\n",
      "Model 32\n",
      "Model 33\n",
      "Model 34\n",
      "Model 35\n",
      "Model 36\n",
      "Model 37\n",
      "Model 38\n",
      "Model 39\n",
      "Model 40\n",
      "Model 41\n",
      "Model 42\n",
      "Model 43\n",
      "Model 44\n",
      "Model 45\n",
      "Model 46\n",
      "Model 47\n",
      "Model 48\n",
      "Model 49\n",
      "Model 50\n",
      "Model 51\n",
      "Model 52\n",
      "Model 53\n",
      "Model 54\n",
      "Model 55\n",
      "Model 56\n",
      "Model 57\n",
      "Model 58\n",
      "Model 59\n",
      "Model 60\n",
      "Model 61\n",
      "Model 62\n",
      "Model 63\n",
      "Model 64\n",
      "Model 65\n",
      "Model 66\n",
      "Model 67\n",
      "Model 68\n",
      "Model 69\n",
      "Model 70\n",
      "Model 71\n",
      "Model 72\n",
      "Model 73\n",
      "Model 74\n",
      "Model 75\n",
      "Model 76\n",
      "Model 77\n",
      "Model 78\n",
      "Model 79\n",
      "Model 80\n",
      "Model 81\n",
      "Model 82\n",
      "Model 83\n",
      "Model 84\n",
      "Model 85\n",
      "Model 86\n",
      "Model 87\n",
      "Model 88\n",
      "Model 89\n",
      "Model 90\n",
      "Model 91\n",
      "Model 92\n",
      "Model 93\n",
      "Model 94\n",
      "Model 95\n",
      "Model 96\n",
      "Model 97\n",
      "Model 98\n",
      "Model 99\n",
      "Model 100\n",
      "Model 101\n",
      "Model 102\n",
      "Model 103\n",
      "Model 104\n",
      "Model 105\n",
      "Model 106\n",
      "Model 107\n",
      "Model 108\n",
      "Model 109\n",
      "Model 110\n",
      "Model 111\n",
      "Model 112\n",
      "Model 113\n",
      "Model 114\n",
      "Model 115\n",
      "Model 116\n",
      "Model 117\n",
      "Model 118\n",
      "Model 119\n",
      "Model 120\n",
      "Model 121\n",
      "Model 122\n",
      "Model 123\n",
      "Model 124\n",
      "Model 125\n",
      "Model 126\n",
      "Model 127\n",
      "Model 128\n",
      "Model 129\n",
      "Model 130\n",
      "Model 131\n",
      "Model 132\n",
      "Model 133\n",
      "Model 134\n",
      "Model 135\n",
      "Model 136\n",
      "Model 137\n",
      "Model 138\n",
      "Model 139\n",
      "Model 140\n",
      "Model 141\n",
      "Model 142\n",
      "Model 143\n",
      "Model 144\n",
      "Model 145\n",
      "Model 146\n",
      "Model 147\n",
      "Model 148\n",
      "Model 149\n",
      "Model 150\n",
      "Model 151\n",
      "Model 152\n",
      "Model 153\n",
      "Model 154\n",
      "Model 155\n",
      "Model 156\n",
      "Model 157\n",
      "Model 158\n",
      "Model 159\n",
      "Model 160\n",
      "Model 161\n",
      "Model 162\n",
      "Model 163\n",
      "Model 164\n",
      "Model 165\n",
      "Model 166\n",
      "Model 167\n",
      "Model 168\n",
      "Model 169\n",
      "Model 170\n",
      "Model 171\n",
      "Model 172\n",
      "Model 173\n",
      "Model 174\n",
      "Model 175\n",
      "Model 176\n",
      "Model 177\n",
      "Model 178\n",
      "Model 179\n",
      "Model 180\n",
      "Model 181\n",
      "Model 182\n",
      "Model 183\n",
      "Model 184\n",
      "Model 185\n",
      "Model 186\n",
      "Model 187\n",
      "Model 188\n",
      "Model 189\n",
      "Model 190\n",
      "Model 191\n",
      "Model 192\n",
      "Model 193\n",
      "Model 194\n",
      "Model 195\n",
      "Model 196\n",
      "Model 197\n",
      "Model 198\n",
      "Model 199\n",
      "Model 200\n",
      "Model 201\n",
      "Model 202\n",
      "Model 203\n",
      "Model 204\n",
      "Model 205\n",
      "Model 206\n",
      "Model 207\n",
      "Model 208\n",
      "Model 209\n",
      "Model 210\n",
      "Model 211\n",
      "Model 212\n",
      "Model 213\n",
      "Model 214\n",
      "Model 215\n",
      "Model 216\n",
      "Model 217\n",
      "Model 218\n",
      "Model 219\n",
      "Model 220\n",
      "Model 221\n",
      "Model 222\n",
      "Model 223\n",
      "Model 224\n",
      "Model 225\n",
      "Model 226\n",
      "Model 227\n",
      "Model 228\n",
      "Model 229\n",
      "Model 230\n",
      "Model 231\n",
      "Model 232\n",
      "Model 233\n",
      "Model 234\n",
      "Model 235\n",
      "Model 236\n",
      "Model 237\n",
      "Model 238\n",
      "Model 239\n",
      "Model 240\n",
      "Model 241\n",
      "Model 242\n",
      "Model 243\n",
      "Model 244\n",
      "Model 245\n",
      "Model 246\n",
      "Model 247\n",
      "Model 248\n",
      "Model 249\n",
      "Model 250\n",
      "Model 251\n",
      "Model 252\n",
      "Model 253\n",
      "Model 254\n",
      "Model 255\n",
      "Model 256\n",
      "Model 257\n",
      "Model 258\n",
      "Model 259\n",
      "Model 260\n",
      "Model 261\n",
      "Model 262\n",
      "Model 263\n",
      "Model 264\n",
      "Model 265\n",
      "Model 266\n",
      "Model 267\n",
      "Model 268\n",
      "Model 269\n",
      "Model 270\n",
      "Model 271\n",
      "Model 272\n",
      "Model 273\n",
      "Model 274\n",
      "Model 275\n",
      "Model 276\n",
      "Model 277\n",
      "Model 278\n",
      "Model 279\n",
      "Model 280\n",
      "Model 281\n",
      "Model 282\n",
      "Model 283\n",
      "Model 284\n",
      "Model 285\n",
      "Model 286\n",
      "Model 287\n",
      "Model 288\n",
      "Model 289\n",
      "Model 290\n",
      "Model 291\n",
      "Model 292\n",
      "Model 293\n",
      "Model 294\n",
      "Model 295\n",
      "Model 296\n",
      "Model 297\n",
      "Model 298\n",
      "Model 299\n",
      "Model 300\n",
      "Model 301\n",
      "Model 302\n",
      "Model 303\n",
      "Model 304\n",
      "Model 305\n",
      "Model 306\n",
      "Model 307\n",
      "Model 308\n",
      "Model 309\n",
      "Model 310\n",
      "Model 311\n",
      "Model 312\n",
      "Model 313\n",
      "Model 314\n",
      "Model 315\n",
      "Model 316\n",
      "Model 317\n",
      "Model 318\n",
      "Model 319\n",
      "Model 320\n",
      "Model 321\n",
      "Model 322\n",
      "Model 323\n",
      "Model 324\n",
      "Model 325\n",
      "Model 326\n",
      "Model 327\n",
      "Model 328\n",
      "Model 329\n",
      "Model 330\n",
      "Model 331\n",
      "Model 332\n",
      "Model 333\n",
      "Model 334\n",
      "Model 335\n",
      "Model 336\n",
      "Model 337\n",
      "Model 338\n",
      "Model 339\n",
      "Model 340\n",
      "Model 341\n",
      "Model 342\n",
      "Model 343\n",
      "Model 344\n",
      "Model 345\n",
      "Model 346\n",
      "Model 347\n",
      "Model 348\n",
      "Model 349\n",
      "Model 350\n",
      "Model 351\n",
      "Model 352\n",
      "Model 353\n",
      "Model 354\n",
      "Model 355\n"
     ]
    }
   ],
   "source": [
    "error_list = []\n",
    "\n",
    "for i in range(1, 356):\n",
    "    print(f'Model {i}')\n",
    "    for d in [10, 20, 30, 40, 60, 120]:\n",
    "       \n",
    "# for error in redo_list:\n",
    "#         i, d = error\n",
    "#         i = int(i)\n",
    "#         d = int(d[:-1])\n",
    "#         print(f'Model {i}_{d}d')\n",
    "        early_stop_count = 0\n",
    "        \n",
    "        train_set = StockDataset(f'./data/training_data_300/training_data_{i}_{d}d.csv')\n",
    "        val_set = StockDataset(f'./data/training_data_300/testing_data_{i}_{d}d.csv')\n",
    "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) #only shuffle the training data\n",
    "        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # the path where checkpoint saved\n",
    "        model_path = f'./models_300/models/model_{i}_{d}d.ckpt'\n",
    "\n",
    "        # create model, define a loss function, and optimizer\n",
    "        model = Classifier().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training\n",
    "\n",
    "        best_acc = 0.0\n",
    "        best_profit_count = 0.0\n",
    "\n",
    "        acc_record = {'train': [], 'dev': []}\n",
    "        macro_f1_record = {'train': [], 'dev': []}\n",
    "        profit_count_record = {'train': [], 'dev': []}\n",
    "\n",
    "        # init accuracy\n",
    "        train_acc = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        train_labels = []\n",
    "        train_predictions = []\n",
    "        val_labels = []\n",
    "        val_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, val_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() \n",
    "                for y in val_pred.cpu().numpy():\n",
    "                    val_predictions.append(y)\n",
    "\n",
    "                for y in labels.cpu().numpy():\n",
    "                    val_labels.append(y)\n",
    "            \n",
    "            for data in train_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs) \n",
    "                _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                train_acc += (train_pred.cpu() == labels.cpu()).sum().item() \n",
    "                for y in train_pred.cpu().numpy():\n",
    "                    train_predictions.append(y)\n",
    "\n",
    "                for y in labels.cpu().numpy():\n",
    "                    train_labels.append(y)\n",
    "\n",
    "            acc_record['dev'].append(val_acc/len(val_set))\n",
    "            acc_record['train'].append(train_acc/len(train_set))\n",
    "            macro_f1_record['dev'].append(f1_score(val_labels, val_predictions, average='macro'))\n",
    "            macro_f1_record['train'].append(f1_score(train_labels, train_predictions, average='macro'))\n",
    "            profit_count_record['dev'].append(profit_count(val_labels, val_predictions))\n",
    "            profit_count_record['train'].append(profit_count(train_labels, train_predictions))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # start training\n",
    "        for epoch in range(num_epoch):\n",
    "        \n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            train_labels = []\n",
    "            train_predictions = []\n",
    "            val_labels = []\n",
    "            val_predictions = []\n",
    "\n",
    "            # training\n",
    "            model.train() # set the model to training mode\n",
    "            for data in train_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad() \n",
    "                outputs = model(inputs) \n",
    "                batch_loss = criterion(outputs, labels)\n",
    "                _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                batch_loss.backward() # compute gradient\n",
    "                optimizer.step() # update model with optimizer\n",
    "\n",
    "                train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "                train_loss += batch_loss.item()\n",
    "                for y in train_pred.cpu().numpy():\n",
    "                    train_predictions.append(y)\n",
    "\n",
    "                for y in labels.cpu().numpy():\n",
    "                    train_labels.append(y)\n",
    "\n",
    "            \n",
    "            acc_record['train'].append(train_acc/len(train_set))\n",
    "\n",
    "            train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "            macro_f1_record['train'].append(train_f1)\n",
    "\n",
    "            train_weighted_precision = profit_count(train_labels, train_predictions)\n",
    "            profit_count_record['train'].append(train_weighted_precision)\n",
    "\n",
    "            # validation\n",
    "\n",
    "            model.eval() # set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    batch_loss = criterion(outputs, labels) \n",
    "                    _, val_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                \n",
    "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() \n",
    "                    val_loss += batch_loss.item()\n",
    "                    for y in val_pred.cpu().numpy():\n",
    "                        val_predictions.append(y)\n",
    "\n",
    "                    for y in labels.cpu().numpy():\n",
    "                        val_labels.append(y)\n",
    "\n",
    "\n",
    "                acc_record['dev'].append(val_acc/len(val_set))\n",
    "\n",
    "                val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "                macro_f1_record['dev'].append(val_f1)\n",
    "\n",
    "                val_profit_count = profit_count(val_labels, val_predictions)\n",
    "                profit_count_record['dev'].append(val_profit_count)\n",
    "\n",
    "\n",
    "                # if the model improves, save a checkpoint at this epoch\n",
    "                if val_profit_count > best_profit_count:\n",
    "                    best_profit_count = val_profit_count\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    # print('[{:03d}/{:03d}] Train Acc: {:3.6f} F1: {:.3f} wP: {:.3f} Loss: {:3.6f} | \\\n",
    "                    # Val Acc: {:3.6f} F1: {:.3f} wP: {:.3f} loss: {:3.6f}'\n",
    "                        # .format(epoch + 1, num_epoch, train_acc/len(train_set), train_f1, train_weighted_precision, train_loss/len(train_loader), \n",
    "                        #                             val_acc/len(val_set), val_f1, val_weighted_precision, val_loss/len(val_loader)))\n",
    "                    # print('saving model with Val Acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "                    # print('saving model with Val Precision {:.3f}'.format(best_macro_precision))\n",
    "                    early_stop_count = 0\n",
    "                else:\n",
    "                    early_stop_count += 1\n",
    "        \n",
    "            # if early_stop_count >= 20:\n",
    "            #     break\n",
    "\n",
    "        plot_learning_curve(acc_record, 'deep model', 'Accuracy', i, d)\n",
    "        plot_learning_curve(profit_count_record, 'deep model', 'Macro Precision', i, d, limit_y=False)\n",
    "        plot_learning_curve(macro_f1_record, 'deep model', 'Macro F1 Score', i, d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "    \n",
    "with open(\"errors.json\", \"w\") as outfile:\n",
    "    json.dump(error_list, outfile)\n",
    "with open('errors.json') as jsonfile:\n",
    "    redo_list = json.load(jsonfile)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average annualized return of top 20 stocks selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "\n",
    "with open('trend_params_300.json') as jsonfile:\n",
    "    code_para_map = json.load(jsonfile)\n",
    "\n",
    "with open('codes_300.txt', 'r') as f:\n",
    "    codes = f.read().split()\n",
    "\n",
    "\n",
    "# predicted_label = {10: [[] for _ in range(315)], 20: [[] for _ in range(305)],\n",
    "#                    30: [[] for _ in range(295)], 40: [[] for _ in range(285)],\n",
    "#                    60: [[] for _ in range(265)], 120:[[] for _ in range(205)]}\n",
    "                  \n",
    "# predicted_value = {10: [[] for _ in range(315)], 20: [[] for _ in range(305)],\n",
    "#                    30: [[] for _ in range(295)], 40: [[] for _ in range(285)],\n",
    "#                    60: [[] for _ in range(265)], 120:[[] for _ in range(205)]}\n",
    "\n",
    "# true_label =      {10: [[] for _ in range(315)], 20: [[] for _ in range(305)],\n",
    "#                    30: [[] for _ in range(295)], 40: [[] for _ in range(285)],\n",
    "#                    60: [[] for _ in range(265)], 120:[[] for _ in range(205)]}\n",
    "\n",
    "max_sample_count = 4000\n",
    "\n",
    "predicted_label = {10: [[] for _ in range(max_sample_count)], 20: [[] for _ in range(max_sample_count)],\n",
    "                   30: [[] for _ in range(max_sample_count)], 40: [[] for _ in range(max_sample_count)],\n",
    "                   60: [[] for _ in range(max_sample_count)], 120:[[] for _ in range(max_sample_count)]}\n",
    "                  \n",
    "predicted_value = {10: [[] for _ in range(max_sample_count)], 20: [[] for _ in range(max_sample_count)],\n",
    "                   30: [[] for _ in range(max_sample_count)], 40: [[] for _ in range(max_sample_count)],\n",
    "                   60: [[] for _ in range(max_sample_count)], 120:[[] for _ in range(max_sample_count)]}\n",
    "\n",
    "true_label =      {10: [[] for _ in range(max_sample_count)], 20: [[] for _ in range(max_sample_count)],\n",
    "                   30: [[] for _ in range(max_sample_count)], 40: [[] for _ in range(max_sample_count)],\n",
    "                   60: [[] for _ in range(max_sample_count)], 120:[[] for _ in range(max_sample_count)]}\n",
    "\n",
    "for i in range(1, 201):\n",
    "  # print(f'model {i}')\n",
    "  for d in [10, 20, 30, 40, 60, 120]:\n",
    "    \n",
    "    model_path = f'./models_300/models/model_{i}_{d}d.ckpt'\n",
    "    # create testing dataset\n",
    "\n",
    "    try:\n",
    "      test_set = StockDataset(f'./data/training_data_300/testing_data_{i}_{d}d.csv')\n",
    "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      for k in range(max_sample_count):\n",
    "        predicted_value[d][k].append(0)\n",
    "        predicted_label[d][k].append(1)\n",
    "        true_label[d][k].append(1)\n",
    "      continue\n",
    "\n",
    "    # create model and load weights from checkpoint\n",
    "    model = Classifier().to(device)\n",
    "    model.load_state_dict(torch.load(model_path), strict=False)\n",
    "\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      days = 0\n",
    "      for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = nn.Softmax(dim=1)(model(inputs))\n",
    "        \n",
    "        test_value, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "              \n",
    "\n",
    "        for j in range(len(outputs)):\n",
    "          predicted_value[d][days + j].append(test_value.cpu().numpy()[j])\n",
    "          predicted_label[d][days + j].append(test_pred.cpu().numpy()[j])\n",
    "          true_label[d][days + j].append(labels.cpu().numpy()[j])\n",
    "            \n",
    "        days += len(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tax_rate = 0.0038\n",
    "annualized_return = {10: [], 20: [], 30: [], 40: [], 60: [], 120: []}\n",
    "roi_long_record = {10: [], 20: [], 30: [], 40: [], 60: [], 120: []}\n",
    "roi_short_record = {10: [], 20: [], 30: [], 40: [], 60: [], 120: []}\n",
    "\n",
    "for d in [10, 20, 30, 40, 60, 120]:\n",
    "    # for day_shift in range(0, 85, 21):\n",
    "    for day_shift in range(0, 127, 21):\n",
    "    # for day_shift in [0, 246, 494, 738, 982, 1228, 1475, 1717]:\n",
    "        compound_interest = 1\n",
    "        for day in range(0, 240, d):\n",
    "            predicted_value_n = predicted_value[d][day + day_shift]\n",
    "            predicted_label_n = predicted_label[d][day + day_shift]\n",
    "            true_label_n = true_label[d][day + day_shift]\n",
    "\n",
    "            sorted_value = []\n",
    "            for i, value in enumerate(predicted_value_n):\n",
    "                sorted_value.append((value, i))\n",
    "            sorted_value = sorted(sorted_value)\n",
    "            \n",
    "            # roi = 0\n",
    "            roi_long = 0\n",
    "            roi_short = 0\n",
    "\n",
    "            # stock_selected_count = 0\n",
    "            long_count = 0\n",
    "            short_count = 0\n",
    "          \n",
    "            long_weight = 0\n",
    "            short_weight = 0\n",
    "\n",
    "            for value, idx in reversed(sorted_value):\n",
    "\n",
    "                if value < 0.50 and long_count >= 5 and short_count >= 5:\n",
    "                    break\n",
    "                # if value < 0.4:\n",
    "                #     break\n",
    "\n",
    "                weight = 1\n",
    "                if predicted_label_n[idx] == 2 and long_count < 10: # long this stock\n",
    "                    long_weight += weight\n",
    "                    if true_label_n[idx] == 2: # hit\n",
    "                        roi_long += weight * (code_para_map[codes[idx]][f'{d}d'][0] - 1)\n",
    "                    elif true_label_n[idx] == 0: # lose\n",
    "                        roi_long -= weight * (code_para_map[codes[idx]][f'{d}d'][0] - 1)\n",
    "\n",
    "                    elif true_label_n[idx] == 1: # stable\n",
    "                        close_ser = pd.read_csv(f'./data/training_data_300/close_df_{idx + 1}.csv')['close']\n",
    "                        \n",
    "                        roi_long += weight * ((close_ser.iloc[day + day_shift + d] / close_ser.iloc[day + day_shift]) - 1)\n",
    "                      \n",
    "                    long_count += 1\n",
    "\n",
    "                elif predicted_label_n[idx] == 0 and short_count < 10: # short this stock\n",
    "                    short_weight += weight\n",
    "                    if true_label_n[idx] == 0: # hit\n",
    "                        roi_short += weight * (1 - code_para_map[codes[idx]][f'{d}d'][1]) * (10 / 9)\n",
    "                    elif true_label_n[idx] == 2: #lose\n",
    "                        roi_short -= weight * (1 - code_para_map[codes[idx]][f'{d}d'][1]) * (10 / 9)\n",
    "\n",
    "                    elif true_label_n[idx] == 1: # stable\n",
    "                        close_ser = pd.read_csv(f'./data/training_data_300/close_df_{idx + 1}.csv')['close']\n",
    "                        \n",
    "                        roi_short += weight * (1 - (close_ser.iloc[day + day_shift + d] / close_ser.iloc[day + day_shift])) * (10 / 9)\n",
    "                    \n",
    "                    short_count += 1\n",
    "\n",
    "                if long_count >= 10 and short_count >= 10:\n",
    "                    break\n",
    "\n",
    "            if long_count < 5 or short_count < 5:\n",
    "                print(long_count, short_count)\n",
    "\n",
    "            roi_long /= long_weight\n",
    "            roi_short /= short_weight\n",
    "            roi_long_record[d].append(roi_long * compound_interest)\n",
    "            roi_short_record[d].append(roi_short * compound_interest)\n",
    "            roi_total = ((roi_long + roi_short) / 2) - tax_rate # roi in the day\n",
    "            compound_interest *= 1 + roi_total\n",
    "\n",
    "        annualized_return[d].append(compound_interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 days :       8.42 ( 5.77 / 2.65 )\n",
      "20 days :       8.7 ( 4.88 / 3.82 )\n",
      "30 days :       9.9 ( 7.5 / 2.4 )\n",
      "40 days :       10.98 ( 7.77 / 3.21 )\n",
      "60 days :       14.12 ( 8.68 / 5.44 )\n",
      "120 days :       4.62 ( 6.48 / -1.86 )\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "for d in [10, 20, 30, 40, 60, 120]:\n",
    "    avg_long = mean(roi_long_record[d])\n",
    "    avg_short = mean(roi_short_record[d])\n",
    "    avg_total = avg_long + avg_short\n",
    "    irr = round((mean(annualized_return[d]) - 1) * 100, 2)\n",
    "    \n",
    "    print(f'{d} days :      ',\n",
    "            irr,\n",
    "            '(',\n",
    "            round((avg_long / avg_total) * irr, 2),\n",
    "            '/',\n",
    "            round((avg_short / avg_total) * irr, 2),\n",
    "            ')'\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2021/01/04\\n~\\n2021/12/27',\n",
      "'2021/02/02\\n~\\n2022/01/26',\n",
      "'2021/03/15\\n~\\n2022/03/08',\n",
      "'2021/04/15\\n~\\n2022/04/08',\n",
      "'2021/05/17\\n~\\n2022/05/10',\n",
      "'2021/06/16\\n~\\n2022/06/09',\n",
      "'2021/07/15\\n~\\n2022/07/08',\n",
      "2.7\n",
      "TAIEX [0.21117324284006922, 0.12146852325976143, 0.035442692098689665, 0.01216919164266228, 0.04609971805190738, -0.039665215688132416, -0.19793847131476372]\n",
      "(10, [1.1081376757837271, 1.073145516612901, 1.1249806183540974, 1.084590053565452, 1.1196103147693244, 1.0803938864142004, 0.9985213152816598])\n",
      "(20, [1.1420753130494048, 1.0842288647821952, 1.018606524512556, 1.0436498117118627, 1.098607776512599, 1.155398976347297, 1.0663343041521918])\n",
      "(30, [1.1547682900947043, 1.146129997518299, 1.1105285072670497, 1.100583336430828, 1.065622244896098, 1.0676532039032038, 1.047378669994937])\n",
      "(40, [1.1189220892526417, 1.1423514213938353, 1.1555334096326584, 1.0498885144465697, 1.0952556230255461, 1.1005930723546864, 1.1061117066056672])\n",
      "(60, [1.1965355290155724, 1.238452619763638, 1.1445046591960328, 1.1431021124648755, 1.0619853700220172, 1.0878558139424008, 1.116081101705455])\n",
      "(120, [1.0949532578197154, 1.1098998012363552, 1.0698618320447109, 1.0959329782704117, 1.000263819145514, 0.9769588191455245, 0.9756820304370739])\n"
     ]
    }
   ],
   "source": [
    "# Baseline TAIEX\n",
    "import pandas as pd\n",
    "\n",
    "baseline = []\n",
    "df = pd.read_csv(f'./data/indicator_data/taiex.csv')\n",
    "df = df[df['date'] >= '2021-01-01']\n",
    "# for day_shift in range(0, 85, 21):\n",
    "for day_shift in range(0, 127, 21):\n",
    "# for day_shift in [0, 246, 494, 738, 982, 1228, 1475, 1717]:\n",
    "    baseline.append((df['TAIEX'].iloc[day_shift + 240] / df['TAIEX'].iloc[day_shift]) - 1)\n",
    "    print('\\'' +str(df.iloc[day_shift]['date']).replace('-', '/') + r'\\n~\\n'  +str(df.iloc[day_shift + 240]['date']).replace('-', '/')+ '\\',')\n",
    "print(round(mean(baseline) * 100, 2))\n",
    "\n",
    "print('TAIEX' ,baseline)\n",
    "for irr in annualized_return.items():\n",
    "    print(irr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.12\n",
      "baseline [0.26537324284006925, 0.17566852325976143, 0.08964269209868966, 0.06636919164266228, 0.10029971805190738, 0.014534784311867582, -0.14373847131476372]\n"
     ]
    }
   ],
   "source": [
    "# 2013~2020\n",
    "# average_dividend_yield = [3.61, 4.12, 4.33, 5.67, 3.78, 5.62, 6.7, 5.61]\n",
    "# 2021\n",
    "average_dividend_yield = [5.42]\n",
    "# 2009~2011\n",
    "# average_dividend_yield = [10.3, 0, 8.54]\n",
    "for i in range(len(baseline)):\n",
    "    baseline[i] += average_dividend_yield[0] * 0.01\n",
    "print(round(mean(baseline) * 100, 2))\n",
    "\n",
    "print('baseline' ,baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAFFCAYAAAAHPiHBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1VUlEQVR4nO3df/zV8/3/8dtdKT/SCqH1Y/m9MikVM+ZbzM/5MLSUH/OrTxszMfPBZz7D9ukjxn7YbGYrjIiPnwlh6BOGlFIpVhTerSURMlnl8f3j9XrnvN/nvN/v867zs/f9ermcy/u8ft/POa/3eZzXr+dLEYGZmVmmTcodwMzMKo+Lg5mZZXFxMDOzLC4OZmaWxcXBzMyyuDiYmVkWFwezJkgaKKkmo/sVSQMLvIybJf13IedptiFcHKziSJos6X1JbcudJZeI2CMiJpdqeZJOk7RW0kpJH0p6WdJReU5bp7CZ5cvFwSqKpB7A14EAji5vmoryXES0AzoAvwPGS+pQ7IVKal3sZVhlcnGwSvMd4HngZuDUzAHprpfrJT0k6SNJL0jaOWN4SPqepPmSVqTjKh12uaTbMsbtkY7fOu0+XdK8dL5vSPpuQwElLZL0jfT5ivQX/UpJH6fz7JEOO0rSzHScv0rqnTGPvpJeSpd3J7BZPm9ORHwG3ApsCeyazqutpGskvSVpqaQbJG0uaUvgEeCLGRm/WH8XVo7dZoskXSRpFvCxpF3S13Vquox3Jf04n7xWvVwcrNJ8BxiXPg6TtH294UOBK4COwAJgVL3hRwEDgN7AEOCwPJf7Tjpte+B04JeS9m5qoojoEBHt0l/1vwaeBhZL6guMBb4LbAP8AZiQfpG3Ae4n+ZLfGvhf4Ph8QkpqleZbDbyZ9h4N7Ab0AXYBugA/iYiPgSOAv9dmjIi/57McYBjwTZItlTVpvwOA3YGDgZ9I6pnnvKwKuThYxZB0APAl4K6ImA68DpxYb7T7ImJqRKwhKSB96g0fHRErIuIt4Kkcw3OKiIci4vVI/B/wGMnurXyzn5BmPT4iVgMjgD9ExAsRsTYibgE+Bb6aPjYFfhURqyPibuDFJhbxVUkrgFXANcDJEfFOumU0Ajg/It6LiI+A/yEpohviuoh4OyI+yeh3RUR8EhEvAy8De23gMqyCuThYJTkVeCwi3k27b6feriXgHxnP/wm0a+bwnCQdIel5Se+lX8JHAtvmOW1f4LfAsRGxLO39JeCCdJfSinSe3YAvpo/FUbfVyzdp3PMR0YFki2kCnxeuTsAWwPSM5UxK+2+It3P0W6/31qqTDzZZRZC0OcluoFaSar+E2gIdJO2V/lrdEB+TfInW2iFj2W2Be0h2aT0QEasl3Q8oj9zbkewi+n5EzMgY9DYwKiLq7/ZC0v8DukhSRoHoTrKl1KiIWCnpLOANSWNJfsF/AuwREYtzTZKjX4PvRRPTWQviLQerFN8C1gK9SHYF9QF6kuzD/04B5j8TOFBSd0lfAC7JGNaGpBAtA9ZIOgI4tKkZpgez7wZui4i76g3+I/A9SfsqsaWkb0raCniOZD/+uZI2lXQcsE++LyQi3gP+RHJc4bN0Wb9MCxWSukiqPdayFNgmfc2Z78WRkraWtANwXr7LtpbDxcEqxanATRHxVkT8o/ZBsrvmpA09pTIiHgfuBGYB04GJGcM+As4F7gLeJzl2MCGP2XYl2b1zXsbZQCsldY+IacC/p/nfJzl4flq6vH8Bx6Xd7wEnAPc28yX9iuQLvjdwUTr/5yV9CPyF5MAxEfEqcAfJlsYKSV8kORD+MrCI5NjKnc1ctrUA8s1+zMysPm85mJlZFhcHMzPL4uJgZmZZXBzMzCyLi4OZmWXZKC6C23bbbaNHjx7ljmFmVlWmT5/+bkTkvJp+oygOPXr0YNq0aeWOYWZWVSQ12GzLRlEcNpjqtZLgaz/MrIXzMQczM8vi4mBmZllcHMzMLIuLg5mZZSlbcZDUTdJTkuZKekXSyLT/5ZIWp/fenSnpyHJlNDNrqcp5ttIa4IKIeClt4366pMfTYb+MiGvKmM3MrEUrW3GIiCXAkvT5R5LmkdwY3czMyqwijjlI6gH0BV5Ie50jaZaksZI6NjDNCEnTJE1btmxZrlHMzGw9lb04SGpHcv/e8yLiQ+D3wM4kt4lcAlyba7qIuDEi+kdE/06dNvRe6mZmlqmsxUHSpiSFYVxE3AsQEUsjYm3GvXHzvreumZkVRjnPVhIwBpgXEb/I6N85Y7RjgTmlzmZm1tKV82yl/YFTgNmSZqb9/hMYJqkPECQ3QP9uOcKZmbVk5Txb6RlAOQY9XOosZmZWl1tltSyTJ9et2QMHupVas5am7GcrmZlZ5XFxMDOzLC4OZmaWxcXBzMyyuDiYmVkWFwczM8vi4mBmZllcHMzMLIuLg5mZZXFxMDOzLC4OZmaWxcXBzMyyuDiYmVkWFwczM8vi4mBmZlnKeZvQbpKekjRX0iuSRqb9t5b0uKT56d+O5cpoZtZSlXPLYQ1wQUT0Ar4KfF9SL+Bi4ImI2BV4Iu02M7MSKltxiIglEfFS+vwjYB7QBTgGuCUd7RbgW2UJaGbWglXEMQdJPYC+wAvA9hGxJB30D2D7cuUyM2upyl4cJLUD7gHOi4gPM4dFRAA5b2AsaYSkaZKmLVu2rARJzcxajrIWB0mbkhSGcRFxb9p7qaTO6fDOwDu5po2IGyOif0T079SpU2kCm5m1EK3LtWBJAsYA8yLiFxmDJgCnAqPTvw+UIV5BTJ6srH4DB+bcEDIzqyhlKw7A/sApwGxJM9N+/0lSFO6SdCbwJjCkPPHMzFqushWHiHgGyP5pnTi4lFnMzKyush+QNjOzyuPiYGZmWVwczMwsi4uDmZllcXEwM7MsLg5mZpbFxcHMzLK4OJiZWZZyXiFtG0I5rh8MN81hZoXhLQczM8vi4mBmZllcHMzMLIuLg5mZZXFxMDOzLC4OZmaWxcXBzMyyuDiYmVmWshYHSWMlvSNpTka/yyUtljQzfRxZzoxmZi1Rua+Qvhn4LfDnev1/GRHXlD6OVY1r610hfoGvDm8pJmtyne6BMbAsOTZ2ZS0OETFFUo9yZlhfd911V1a/IUOGlCGJmVnhVeoxh3MkzUp3O3Usdxgzs5amEovD74GdgT7AEuDaXCNJGiFpmqRpy5YtK2E8M7ONX8UVh4hYGhFrI+Iz4I/APg2Md2NE9I+I/p06dSptSDOzjVzexxzS3TtfBD4BFqVf3gUnqXNELEk7jwXmNDa+NU/9YyU+TmJmuTRaHCR9Afg+MAxoAywDNgO2l/Q88LuIeGp9Fy7pDmAgsK2kGuAyYKCkPkAAi4Dvru/8reWYPDn7/hYDB/oMJrP11dSWw90kp5l+PSJWZA6Q1A84RdJOETFmfRYeEcNy9F6veRVb/dPnuLMsMQqv/imhAP1KH6Olql/UXNCsUjRaHCLikEaGTQemFzyRmZmVXbOuc5DUCRgJbA7cEBHzi5LKzMzKqrlnK10LPArcB9xe+DhmZlYJGi0Okh6VdGBGrzYkB4kXAW2LF8vMzMqpqS2HIcC/SbpD0s7AfwFXAr8Gzi52ODMzK4+mDkh/AFwoaSdgFPB34Jz6Zy6ZmdnGpanrHHYGzgL+BVxA0qzFnZIeAq6PiLXFj2hmZqXW1G6lO4B7gaeAWyPi6Yg4DFgBPFbkbGZmViZNncraFlgItAO2qO0ZEX+W9L/FDGYtg66oexFYXOaLwMwqQVPF4WySm/H8C/he5oCI+KRYoczMrLyaOiD9LPBsibKYmVmFaOqA9IPAH4BHI2J1vWE7AaeRtNA6tmgJq0n9dorcRpFVEO/Cs+ZoarfSvwM/BH4t6T0+b5W1B/A68NuIeKCoCc3MrOSa2q30D+A/gP9I7/XcmeR+Dn+LiH8WP56ZWctVzlZ78254LyIWkTSbYWZmG7mKu02omZmVX7Oa7LaWSZMnZ/WLgQNLtvz6tzaFpNEvMyueshYHSWOBo4B3IuIrab+tSe6z1oNkN9aQiHi/pLmuyL472lOs991QzcyqTl7FQdL+wOXAl9JpBERE7LSBy7+Z5CK7P2f0uxh4IiJGS7o47b5oA5djZlbZKuyWvfluOYwBzie5LWjBGtuLiCnpWVCZjgEGps9vASbj4mBmVlL5FocPIuKRoib53PYRsSR9/g9g+xIt18zMUvkWh6ck/ZykhdZPa3tGxEtFSfX5/ENSzhN7JY0ARgB07969mDHMmi3nQfQhJTyMrhy7KC4v3eLLrv4umgt8NXhz5Vsc9k3/9s/oF8BBhY0DwFJJnSNiiaTOwDu5RoqIG4EbAfr37+9P3qyZJmtyVr+BMbDkOawyNVkcJLUCJkTEL0uQB2ACcCowOv3r5jmsxSj3acNZch0k9a/wFqHJ4hARayUNAwpeHCTdQXLweVtJNcBlJEXhLklnAm+ykZ3SXv+fv6z/+GZmDch3t9Kzkn5Lcv3Bx7U9N/SYQ0QMa2DQwRsyXzMz2zD5Foc+6d+fZvQr1jEH21i19IOkZlUkr+IQEYOKHcTMrFjqt24KpW3htBrle4X0T3L1j4if5upvZmbVLd/dSh9nPN+MpD2keYWPY4WU61RF7ix5jI1Orra3fFc129jku1vp2sxuSdcAjxYlkZmZld36tsq6BdC1kEHMzOpzC8nlk+8xh9kkZycBtAI6AT8rVigzMyuvfLccjsp4vgZYGhFripDHzMwqQL63Cf3viHgzfSyOiDWSbi1qMjMzK5t8txz2yOyQ1Jqy3obCcqm/f9b7Zs1sfTW65SDpEkkfAb0lfSjpo7R7KW4Qz8xso9XolkNEXAlcKenKiLikRJnMCsKNHJqtv3x3K/1Y0snAjhHxM0ndgM4RMbWI2cwqQ/02oS4vSwprQM4bK5UhR2Oq8d4Z+RaH64HPSBra+xmwMu03oEi5zKwM6n/RVtqXrJVO3neCi4i9Jc0AiIj3JbUpYi4zMyujfIvD6vSOcAEgqRPJloSZtTBu4bRlyPc6h+uA+4DtJI0CngGuLFoqMzMrq3wb3hsnaTrJHdoEfAt4q4i5kLQI+AhYC6yJiP7FXJ6ZmX2uyeIgqQvQGZgVEa9K2g44DzgN+GJR08GgiHi3yMswM7N6mroI7jxgJvAb4HlJw0nu47A5vkLazGyj1dSWwwhg94h4T1J34G/A/hExvfjRCOAxSQH8ISJuLMEyzcyMpovDqoh4DyAi3pL0WokKA8ABEbE43Y31uKRXI2JK7UBJI0iKF927dy9RJLMNcG32WT7e/rZK1VRx6CrpuozuzpndEXFucWJBRCxO/74j6T5gH2BKxvAbgRsB+vfv7/PozKyqVPoFh00VhwvrdZdkq0HSlsAmEfFR+vxQ4KelWLaZmTXd8N4tpQpSz/bAfUratGkN3B4Rk8qUxaxJWW3n3FmWGGYFs773kC6qiHgD2KvcOczMWqp8r5A2M7MWpKnrHK5K/367NHHMzKwSNLXlcKSSHf++0Y+ZWQvS1DGHScD7QDtJH5K0qxS1fyOifZHzmZlZGTS65RARF0ZEB+ChiGgfEVtl/i1NRDMzK7V8W2U9RtL2fH7ntxciYlnxYpmZWTnldbZSekB6KvBtkgv5pkoaXMxgZmZWPvle53ApMCAi3oF1d4L7C3B3sYKZmVn55Hudwya1hSG1vBnTmplZlcl3y2GSpEeBO9LuE4CHixPJzMzKLd8D0hdKOg44IO11Y0TcV7xYZmZWTnm3rRQR9wL3FjGLWXYDduBG7MzKwMcNzMwsi4uDmVUOqe7Dyibv4iBpc0m7FzOMmZlVhnwvgvs3YCZJW0tI6iNpQhFzmZlZGeW75XA5yT2cVwBExExgx6IkMjOzssu3OKyOiA/q9YtCh8kk6XBJr0laIOniYi7LzMzqyrc4vCLpRKCVpF0l/Qb4a7FCSWoFXA8cAfQChknqVazlmZlZXfkWhx8AewCfArcDHwAjixWKZBfWgoh4IyL+BYwHjini8szMLEO+xeGbEfHjiBiQPi4Fji5iri7A2xndNWk/MzMrAUU0fehA0ksRsXdT/QoWKmkO/PCIGJ52nwLsGxHnZIwzAhgB0L17935vvvlmMaKY2UZKkyfX6Y6BA9dzRtnXY+jyut1xWWEO0dbPDBuQG5A0PSL65xrWaPMZko4AjgS6SLouY1B7YM16J2raYqBbRnfXtN86EXEjcCNA//79i3pw3MysOQpVDMqpqbaV/g5MI9mFND2j/0fA+cUKBbwI7CppR5KiMBQ4sYjLMzOzDI0Wh4h4GXhZ0u0RsbpEmYiINZLOAR4FWgFjI+KVUi3fzKyly7dV1h6SriQ5rXSz2p4RsVNRUiXzfhjfM8LMrCzyPVvpJuD3JMcZBgF/Bm4rVigzMyuvfIvD5hHxBMnZTW9GxOXAN4sXy8zMyinf3UqfStoEmJ8eC1gMtCteLDMzK6d8txxGAlsA5wL9gFOA7xQrlJmZlVe+95B+MX26Ejg9bftoKPBCsYJtqNWrV1NTU8OqVavKHcU2Mpttthldu3Zl0003LXcUs6Jp6iK49sD3SZqumAA8nnZfAMwCxhU74Pqqqalhq622okePHsh3lLICiQiWL19OTU0NO+7oVutt49XUbqVbgd2B2cBw4Cng28CxEVHRDeGtWrWKbbbZxoXBCkoS22yzjbdIbaPX1G6lnSJiTwBJfwKWAN0joir+M1wYrBi8Xm0cNqRNonIpZeamthzWXRUdEWuBmmopDGZmtv6a2nLYS9KH6XMBm6fdAiIi2hc1XSEV+tdeHq3ZLlq0iKOOOoo5c+YUdtnA5MmTueaaa5g4cSITJkxg7ty5XHxxBdww79oCv88X5N+A2dq1a+nfvz9dunRh4sSJLFy4kKFDh7J8+XL69evHrbfeSps2bbKma9euHStXrixkarOq1+iWQ0S0ioj26WOriGid8bx6CsNG7uijj66MwlBmv/71r+nZs+e67osuuojzzz+fBQsW0LFjR8aMGVPGdGbVJd/rHGw9rVmzhpNOOomePXsyePBg/vnPf/LTn/6UAQMG8JWvfIURI0ZQe0+N6667jl69etG7d2+GDh0KwMcff8wZZ5zBPvvsQ9++fXnggQeylnHzzTdzzjnJrS5OO+00zj33XL72ta+x0047cffdd68b7+c//zkDBgygd+/eXHbZZSV49aVTU1PDQw89xPDhw4HkrKInn3ySwYMHA3Dqqady//33A7Bw4UL2228/9txzTy699NJyRTaraC4ORfbaa69x9tlnM2/ePNq3b8/vfvc7zjnnHF588UXmzJnDJ598wsSJEwEYPXo0M2bMYNasWdxwww0AjBo1ioMOOoipU6fy1FNPceGFF/Lxxx83uswlS5bwzDPPMHHixHVbFI899hjz589n6tSpzJw5k+nTpzNlypTivvgSOu+887j66qvZZJNklV6+fDkdOnSgdetkz2nXrl1ZvDi5JcjIkSM566yzmD17Np07dy5bZrNK5uJQZN26dWP//fcH4OSTT+aZZ57hqaeeYt9992XPPffkySef5JVXktbIe/fuzUknncRtt9227kvtscceY/To0fTp04eBAweyatUq3nrrrUaX+a1vfYtNNtmEXr16sXTp0nXzeeyxx+jbty977703r776KvPnzy/iKy+diRMnst1229GvX7+8xn/22WcZNmwYAKecckoxo5lVrXzbVrL1VP+0R0mcffbZTJs2jW7dunH55ZevO2f+oYceYsqUKTz44IOMGjWK2bNnExHcc8897L777nXmU/uln0vbtm3XPa/dZRURXHLJJXz3u98t1EurGM8++ywTJkzg4YcfZtWqVXz44YeMHDmSFStWsGbNGlq3bk1NTQ1dunx+G3KfjmrWOG85FNlbb73Fc889B8Dtt9/OAQccAMC2227LypUr1x0T+Oyzz3j77bcZNGgQV111FR988AErV67ksMMO4ze/+c26L/kZM2asV47DDjuMsWPHrjsrZ/Hixbzzzjsb+vIqwpVXXklNTQ2LFi1i/PjxHHTQQYwbN45Bgwate39vueUWjjkmuW5z//33Z/z48QCMG1exF/mblVXL2XLI49TTYth99925/vrrOeOMM+jVqxdnnXUW77//Pl/5ylfYYYcdGDBgAJCchnnyySfzwQcfEBGce+65dOjQgf/6r//ivPPOo3fv3nz22WfsuOOO645RNMehhx7KvHnz2G+//YDk9M3bbruN7bbbrqCvtzmnnhbbVVddxdChQ7n00kvp27cvZ555JpCc1XTiiSdy1VVXrSsYZlaXokxfmg2RdDnw78CytNd/pneFa1D//v1j2rRpdfrNmzevzmmNZoXk9cvWybWLssK+VxsiaXpE9M81rFK3HH4ZEdeUO4SZWUvlYw5mZpalUovDOZJmSRorqWO5w5iZtTRlKQ6S/iJpTo7HMcDvgZ2BPiStwF7bwDxGSJomadqyZctyjWJmZuupLMccIuIb+Ywn6Y9AzlNzIuJG4EZIDkgXLp2ZmVXcbiVJme0ZHAsUvklTMzNrVCWerXS1pD5AAIuAglzSqysKe0VsXNb0xsoZZ5yxrmmH2ma733vvPU444QQWLVpEjx49uOuuu+jYMfuwSo8ePZg2bRrbbrttQXMX2+TJhX2fBw7Mb6NwxYoVDB8+nDlz5iCJsWPHsvvuu2/U77VZMVXclkNEnBIRe0ZE74g4OiKWlDvT+jrttNOYNGlSnX6jR4/m4IMPZv78+Rx88MGMHj26TOk2LiNHjuTwww/n1Vdf5eWXX6Znz55+r802QMUVh43JgQceyNZbb12n3wMPPMCpp54K1G1Gevny5Rx66KHsscceDB8+nEq7OLGSffDBB0yZMmXdFdBt2rShQ4cOfq/NNoCLQ4ktXbp0XTPRO+yww7oG9K644goOOOAAXnnlFY499tgmW161zy1cuJBOnTpx+umn07dvX4YPH87HH3/s99psA7g4lJGkda2DTpkyhZNPPhmAb37zmzn3jVtua9as4aWXXuKss85ixowZbLnlllm7kPxemzWPi0OJbb/99ixZkhxGWbJkSeEbvmuBunbtSteuXdl3330BGDx4MC+99JLfa7MN4OJQYkcffTS33HILULcZ6QMPPJDbb78dgEceeYT333+/bBmrzQ477EC3bt147bXXAHjiiSfo1auX32uzDRERVf/o169f1Dd37tysfqU2dOjQ2GGHHaJ169bRpUuX+NOf/hTvvvtuHHTQQbHLLrvEwQcfHMuXL4+IiHfffTcOOeSQ6NWrVwwfPjy6d+8ey5YtK/MrqB4zZsyIfv36xZ577hnHHHNMvPfee0V9ryth/bIKkbTBWvdRJYBp0cD3asU12b0+3GS3lZrXL1tnI22y27uVzMwsi4uDmZllcXEwM7MsLg5mZpbFxcHMzLK4OJiZWZZKbLK7KCZrckHnNzAGNjnO22+/zXe+8x2WLl2KJEaMGMHIkSM36ma7NXlyQecXAwc2OU6uptEvvPBCHnzwQdq0acPOO+/MTTfdRIcOHQC48sorGTNmDK1ateK6667jsMMOy5rn5ZdfTrt27fjRj35UyJdjVjW85VBErVu35tprr2Xu3Lk8//zzXH/99cydO9dNSRdYrqbRDznkEObMmcOsWbPYbbfduPLKKwGYO3cu48eP55VXXmHSpEmcffbZrF27thyxzSqai0MRde7cmb333huArbbaip49e7J48WI3JV1guZpGP/TQQ2ndOtkw/upXv0pNTQ2QNJk+dOhQ2rZty4477sguu+zC1KlTARg1ahS77bYbBxxwwLqmOMxaKheHElm0aBEzZsxg3333dVPSJTZ27FiOOOIIABYvXky3bt3WDevatSuLFy9m+vTpjB8/npkzZ/Lwww/z4osvliuuWUUoS3GQ9G1Jr0j6TFL/esMukbRA0muSsncGV6GVK1dy/PHH86tf/Yr27dvXGeampItr1KhRtG7dmpNOOqnR8Z5++mmOPfZYtthiC9q3b8/RRx9dooRmlalcWw5zgOOAKZk9JfUChgJ7AIcDv5PUqvTxCmf16tUcf/zxnHTSSRx33HGAm+0ulZtvvpmJEycybty4dQW4S5cuvP322+vGqampoUuXLuWKaFaxylIcImJeROTaqXsMMD4iPo2IhcACYJ/SpiuciODMM8+kZ8+e/PCHP1zX301JF9+kSZO4+uqrmTBhAltsscW6/kcffTTjx4/n008/ZeHChcyfP5999tmHAw88kPvvv59PPvmEjz76iAcffLCM6c3Kr9JOZe0CPJ/RXZP222D5nHpaaM8++yy33nore+65J3369AHgf/7nf7j44osZMmQIY8aM4Utf+hJ33XUXAJdddhnDhg1jjz324Gtf+xrdu3cveeYNlc+pp4U2bNgwJk+ezLvvvkvXrl254ooruPLKK/n000855JBDgOSg9A033MAee+zBkCFD6NWrF61bt+b666+nVatW7L333pxwwgnstddebLfddgwYMKDkr8OskhStyW5JfwF2yDHoxxHxQDrOZOBHETEt7f4t8HxE3JZ2jwEeiYi7c8x/BDACoHv37v3efPPNOsPdpLIVk9cvW2cjbbK7aFsOEfGN9ZhsMdAto7tr2i/X/G8EboTkfg7rsSwzM2tApZ3KOgEYKqmtpB2BXYGpZc5kZtbilOtU1mMl1QD7AQ9JehQgIl4B7gLmApOA70eEL181MyuxshyQjoj7gPsaGDYKGFXaRGZmlqnSdiuZmVkFcHEwM7MslXadQ9HUXktQKEOGDGlynFWrVnHggQfy6aefsmbNGgYPHswVV1zBwoULGTp0KMuXL6dfv37ceuuttGnTJmv6du3asXLlyoLmNjPLh7cciqht27Y8+eSTvPzyy8ycOZNJkybx/PPPc9FFF3H++eezYMECOnbsyJgxY8od1cysDheHIpJEu3btgKSNpdWrVyOJJ598ksGDBwN1m+xeuHAh++23H3vuuSeXXnppuWKbmbk4FNvatWvp06cP2223HYcccgg777wzHTp0WHevgdomowFGjhzJWWedxezZs9c16W1mVg4uDkXWqlUrZs6cSU1NDVOnTuXVV19tcNxnn32WYcOGAXDKKaeUKqKZWRYXhxLp0KEDgwYN4rnnnmPFihWsWbMGyG4yWrnaaTEzKzEXhyJatmwZK1asAOCTTz7h8ccfp2fPngwaNIi7707aEsxssnv//fdn/PjxAIwbN64smc3MoAWdyprPqaeFtmTJEk499VTWrl3LZ599xpAhQzjqqKPo1asXQ4cO5dJLL6Vv376ceeaZAPz617/mxBNP5KqrrlpXMMzMyqFoTXaXUv/+/WPatGl1+rlJZSsmr1+2zkbaZLd3K5mZWRYXBzMzy7JRF4eNYZeZVR6vV9YSbLTFYbPNNmP58uX+R7aCigiWL1/OZpttVu4oZkW10Z6t1LVrV2pqali2bFm5o9hGZrPNNqNr167ljmFWVBttcdh0003Zcccdyx3DzKwqles2od+W9IqkzyT1z+jfQ9InkmamjxvKkc/MLG8R2Y+NQLm2HOYAxwF/yDHs9YjoU9o4ZmaWqVz3kJ4HbkfIzKxSVeIxhx0lzQA+BC6NiKdzjSRpBDAi7Vwp6bUS5dsWeLdEyyqUaswM1Zm7GjNDdeZ25g33pYYGFK35DEl/AXbIMejHEfFAOs5k4EcRMS3tbgu0i4jlkvoB9wN7RMSHRQm5HiRNa+hy80pVjZmhOnNXY2aoztzOXFxF23KIiG+sxzSfAp+mz6dLeh3YDZjW6IRmZlZQFXURnKROklqlz3cCdgXeKG8qM7OWp1ynsh4rqQbYD3hI0qPpoAOBWZJmAncD34uI98qRsRE3ljvAeqjGzFCduasxM1Rnbmcuoo2iyW4zMyusitqtZGZmlcHFwczMsrTY4iCpm6SnJM1Nm/IYmfbfWtLjkuanfzum/U+SNEvSbEl/lbRXxrzGSnpH0pwcy/mqpD+25NzVmLlaczuz14+CiYgW+QA6A3unz7cC/gb0Aq4GLk77XwxclT7/GtAxfX4E8ELGvA4E9gbm5FjOFcDxLTl3NWau1tzO7PWjYK+v1Aus1AfwAHAI8BrQOePDfy3HuB2BxfX69Wjgg30G+IJzV3fmas3tzF4/1vfRYncrZZLUA+gLvABsHxFL0kH/ALbPMcmZwCN5zHdbYHVEfFCgqPXn34Mqy12NmdP596DKcjtznfl6/WimSmxbqaQktQPuAc6LiA+V0RhgRISkqDf+IJIP9oA8Zn8o8FgB42bmqLrc1Zg5zVF1uZ05i9ePZmrRWw6SNiX5UMdFxL1p76WSOqfDOwPvZIzfG/gTcExELM9jEUcAkwqbujpzV2PmNEfV5Xbm0mROc1Rl7ny02OKgpLyPAeZFxC8yBk0ATk2fn0qyHxFJ3YF7gVMi4m95zr83MLOAsasydzVmzphvVeV25tJkzphv1eXOW6kPclTKg2STLoBZ6Zs/EzgS2AZ4ApgP/AXYOh3/T8D7GeNOy5jXHcASYDVQQ7LJ2B+42bmrM3O15nZmrx+Ferj5jCKRdCmwICLGlztLc1Rj7mrMDNWZ25lLp9y5XRzMzCxLiz3mYGZmDWvRxUHS4ZJek7RA0sVpv3PS7kjPMc4cf1NJLzU0bb1xr5O0soS5x6X95ii5FH/T5uRubPoiZh4j6WUlTQrcnZ4SWDt+Z0mPpc8nSVohaWK9ed4saaGkmemjT7EzZwzL+nwrIXNDuRtbbr31I2czDpLuzJh2kZJm9YudWZJGSfqbpHmSzm1O5nTYDyS9qqR5i6sLmbmR3E9nvFd/l3R/c3JLulzS4ox5HFno3Hkp18GOcj+AVsDrwE5AG+Blkkvf+5JcqbgI2LbeNIOA3zQ0bcZ4/YFbgZUlzH0koPRxB3BWc3I3Nn0RM7fPGOcXpE0OpN2nAxekzw8G/g2YWG++NwODS7l+NPb5ljtzE+91g8utXT/S5w0245Ax/rXAT0qQ+XTgz8Am6XjbNSdzOs5fgLb1py/2OpIxzj3Ad5qZ+3KS2ycXZR3J99GStxz2ITnY80ZE/AsYT3Lu8YyIWNTANIeTXNWYc1oAJXey+znwHyXO/XCkgKlA1+bkbmL6YmX+ENadsrc5yZkf9TMTEU8AHxUwTz5yZm7i8y13ZmjkM25EZu4pQIM32Eo/qyEkPyAKpaHMZwE/jYjP0mzvZEyTT+azgNGR3H64/vTFzA2ApPbAQcD9zcxdEVpycegCvJ3RXZP2a8wgYHIT054DTIjPL58vtEZzp7uDTqHuhTP55G5s+qJllnQTSRMDXybZuqktsLtHxNw85j0q3S31S0ltS5A55+dbIZmh8c+4oeXWrh/5+DqwNCLmb3DSzzWUeWfgBEnTJD0iadeMcfLJvBvwdUkvSPo/SQMKmBma/n/6FvBE7Y+gVL7v9TnpZzVWaauupdaSi0OzSOoCvBcR/2xknC8C3yb9kiuT3wFTIuLpNFOTuRubvtgi4nTgi8A84IS0974kbdQ05RKSojIA2Bq4qBgZM2xBw59vpWZudLnrsX4Mo7BbDY1pC6yKiP7AH4Gx0KzMrUle61eBC4G70i2fUqnzXjUj9+9JCmMfkmsfri1WwMa05OKwGOiW0d017deQw4Hae103NG1fYBdggaRFwBaSFhQqcBPLRtJlQCfgh83MTSPTFzUzQESsJdkkPz7tlVeTARGxJN0T9ilwE8lmfqHkyvw6DX++lZAZGnivG1lu5vrRKEmtgeOAOwuYFxpeP2pIrigGuI/kamHIP3MNcG/6uqcCnwHbNjFNczT2v7gtyXv8UMbwvHJHxNKIWJvuTvsjhV9H8tKSi8OLwK6SdpTUBhhKctl7Q9btK2xo2oh4KCJ2iIgeEdED+GdE7FKK3JKGA4cBw2r30eabG6CR6YuZeZd02QKOBl5Nxz+Y5EBio/R5+zUi2YTPOlulwJnvb+TzrYTMDeWe0MhyM9ePpnwDeDUiagobucH18n6S3TAA/4/kfgmQf+Z100vajeSg8bsFS934d8hgkpMRVmWMn1fu2s8qdSyFX0fyU4qj3pX6IDlD528kvwh/nPY7l+QXxxrg7ySXvLcCZjQ1bY75F/xspUZyr0m7Z6aPnzQnd67pi5mZ5IfJs8BskpV/HNCeZMvlyXrTPg0sAz5JP5vD0v5PZkx/G9Cu2O9zrs+3kjI3sn5kLbeB9SOrGYeMYTcD3yvhOt2B5Jf3bOA5YK/mZCYpBrelr/kl4KBS5E77TwYOz+huTu5b09c8i6TYdC7Ge97Uw1dI50HSAcDJEfG9cmdpjmrMLelkoGtEjC53lnxVY2ao2vWj6jJDdeZ2cTAzsywt+ZiDmZk1wMXBzMyyuDiYmVkWFwczM8vi4mBmZllcHPIg6fy0fZanJZ0haVdJP5K0X7mzNaQaM0N15nbm0qnG3NWYGVwc8rU9sD8wnOSKywdJLtjKpy2dcqnGzFCduZ25dKoxdzVm9nUOZmaWzVsOZmaWxcXBzMyyuDiYmVkWFwczM8vi4mBmZllabHGQ1E3SU5LmSnpF0si0/9aSHpc0P/3bMe1/kpJ7us6W9FdJe2XMa6ykdyRl3ZRD0lcl/bHScjc0n2LkrsbM1Zrbmb1+FEw5biJRCQ+gM7B3+nwrkht29AKuBi5O+18MXJU+/xrQMX1+BPBCxrwOBPYG5uRYzhXA8ZWWu6H5FCN3NWau1tzO7PWjYK+v1Aus1AfwAHAI8BrpnZfSD+21HON2JLkvb2a/HuQuDs8AX6jU3PXnU4rc1Zi5WnM7s9eP9X202N1KmST1APqSXLG4fUQsSQf9g+TqxvrOJL97wW4LrI6IDwoUtf78e1CA3PXmU9Tc1Zg5x/KqIrcze/3YIKWuRpX2ILmX7nTguLR7Rb3h79frHgTMA7ap178H9bYcgBOBSyo8d535FDN3NWau1tzO7PVjg19XORZaKQ9gU+BR4IcZ/RrcJAR6k9xIfLcc8+pBdnG4FehbqblzzadYuasxc7XmdmavHwV5beVYaCU8AAF/Bn5Vr//PqXsw6er0eXdgAfC1BubXg4zikM7/ZdL2qyotdyPzKXjuasxcrbmd2etHwV5fORZaCQ/gACCAWcDM9HEksA3wBDAf+AuwdTr+n4D3M8adljGvO4AlwGqghmR/Yn/g5krN3ch8Cp67GjNXa25n9vpRqIdbZS0SSZcCCyJifLmzNEc15q7GzFCduZ25dMqd28XBzMyy+FRWMzPL4uJgZmZZWnRxUI42kST9XNKraRso90nqUG+a6ZK+IOmhdLxXJI3OGP5LSTPTx98krShC7sMlvSZpgaSL037npN2RXjiTOf6mkl7Kow2XH2S8pqtLkHmMpJfT9/puSe0yxu8s6bH0+alpOzXzJZ2aMc6wtJ2aWZIm1X/dxcqdMew6SSvr9St77gbe65slLcxYN/tkjL+ppJfS5znbCVMD7QUVObMkjUr/j+ZJOreBzDk/I0kHpev9HEm3SGpdyMyN5H46433+u6T7m5n74DT3TEnPSNql0LnzUq4j4ZXwIEebSMChQOv0+VWk7aKk3TsCE4AtgEFpvzbA08AROeb/A2BsgTO3IjlPeqd02S+TtOfSl+R02kXAtvWmGQT8hkbacEnH+QvQNu3ergSZ22eM8wvS0//S7tOBC4CtgTfSvx3T5x2B1sA7ta+VpD2by0vxXqfD+pOcg76y3jRlzd3Ie30zMLiBaQYBv2nofyIjZ1Z7QUXOfDrJKZ6b1F8nM9bphqbdBHib9HoC4KfAmaVaPzLGuQf4Tr6503H+BvRMn59Nmc5YatFbDhExBXivXr/HImJN2vk80DVj8OHApIj4Z0Q8lY7/L+CleuPVGkZymmsh7UNyBsMb6bLHA8dExIyIWNTANIcDj0TEkoh4Kc39EclVml3Scc4CRkfEp+nwd0qQ+UNIfiECm5OczlcnM3AY8HhEvBcR7wOPp8OUPrZMp28P/L2AmRvMLakVybns/5FjmnLnzpm5iWlqM+f8n0gdA9ySPr8F+FZB0iYaynwW8NOI+CzNlrlO1mZuaNptgH9FxN/S8R8Hji9g5sZyAyCpPXAQcH8zckPyf9A+ff4FCr9e56VFF4c8nEHd9k8OByZljqBkt9O/kZzXnNn/SyRbGk8WOFMXkl9EtWr4/Au+IYOAyZk9VK8NF2A34OuSXpD0f5IGFCRtosHMkm4iaX/myyS/qEi/fHePiLkNTRsRq0m+PGaT/PP0AsYUMHNjuc8BJsTn7edQQbkbWz9GpbuyfimpbcY4WetHDvm0F7S+Gsq8M3CCpGmSHpG0a8Y4tZkbmvZdoLWk/mn/wUC3AmZuLHetbwFP1P4ISjWVG2A48LCkGuAUYDRl4OLQAEk/BtYA49LuNkDXiHgjY5zWJFsG12X2Tw0F7o6ItSWKnJOkLsB7EfHPjH7tSDZ3z8tYcVuT7AL5KnAhcFf6y7aoIuJ04IskWzEnpL335fOilZOkTUm+ZPum088CLile0nW2AL5NWsjqqeTcl5AU4AEkn/NFaZ6s9aMpkezvKMU58G2BVRHRH/gjMBbyy5xmHAr8UtJU4COg1P+LdfYcNOO9Ph84MiK6AjeR7HItOReHHCSdBhwFnJSuZABfJ2k6N9ONwPyI+FWO2Qyl8LuUABZT9xdQ17RfQw4nabMFWPfldA8wLiLuzRivBrg3ElOBz4BCHShtNHNaQMfz+Wb/EXy+hdbQtH3SaV9PP6O7SNrLL6Rcy34d2AVYIGkRsIWkBRWUO+dy012Kke42vIlktwbUWz8asVRSZ0gOupMcNylqZtJ1Mu13H0m7RFA3c4PrVkQ8FxFfj4h9gCkk+/ILqcFlpycZ7AM8lDG8ydySOgF7RUTtj4w7Kfx6nZ9yHOiopAfZbSIdDswFOtUb7+fA4Rnd/03yJbtJjnl+meTAcMHbRCH5hf8GyS6r2gNZe2QMX0TGAWngf4Evp89ztuGSDvseyf5dSHYxvV2o/A1lBnbJyHUNcE3a/Vdgq/T51sBCkoO5HdPnW5P86l5S+zkBPwOuLeV7nY6zMuN52XM38l7XNgQn4Fckx5fqrB8N/U9krP9Z7QUVOfNo4Ix0nIHAiznW6QY/I9ID2CRbIE8AB5Vq/Uj/n26pN36TudP+7/L5gfQzgXsKmTvv11eOhVbKg9xtIi0g+WKcmT5uSMd9Edg8fd6VZLN6XsZ4wzPme3ntP1+Rch9J8ivodeDHab9z09ewhmRf9p9IzoiYkTFdzjZc0mFtgNuAOSQH2Av9j1QnM8lW67Mk+97nkOy+aw90Ap6sN+0Z6eeyADg9o//30s9gFvAg9ZpALtZ7XW/4yvRvxeRuYP14MuO9vo2keeg660dD/xNp/5ztBRU5cweSX96zgeeAvRrInPMzIilo80haST2v0OtGE8ueTN0fk83JfWz6ml9O57NTMbI39XDzGXmQ1BX4Y0QcUe4szSHpAODkiPheubPkS9LJJMd2ynIQbn1VY+4qXT+qLjNUZ24XBzMzy+ID0mZmlsXFwczMsrg4mJlZFhcHMzPL4uJgLY6ktWmLl68oaRX2AkmN/i9I6iHpxA1Y1hxJ/ytpi2ZM+0VJdzdzeZMzmowwW28uDtYSfRIRfSJiD+AQkiubL2timh5As4tDxrK+AvyL5BqHJklqHRF/j4jB67FMsw3m4mAtWiQtfY4AzknvH9AjbY//pfRR23TBaJKGCWdKOl9SKyX3/ngxbczuu3ks7mlgF0lbKrlvwlRJMyQdA0mzLZImSHoSeCLNMicdtpmkm5TcB2KGpEFp/80ljVdyv4P7SFq3NdtgBb/5hVm1iYg30hZVtyNpM+iQiFiVtgJ6B8m9Gy4GfhQRRwFIGgF8EBED0hZOn5X0WEQszLWMtJHG2raXfkxyNfUZaau+UyX9JR11b6B3RLyXtpxb6/tJ1NhT0peBxyTtRtKI3z8joqek3iRXt5ttMBcHs7o2BX6r5E5pa0namcrlUKC3pNrdPl8AdiVpQynT5pJmps+fJmme+6/A0ZJ+lPbfDOiePn88InLdT+EA0pZgI+JVSW+m2Q4Erkv7z5I0K8/XadYoFwdr8STtRFII3iE59rCUpB2fTYBVDU0G/CAimmrR9JOI6FNveQKOj4jX6vXfF/i42S/ArAh8zMFatLSJ5BuA30bSlswXgCWR3H3sFJIG0yC5H8BWGZM+CpyVNoGOpN0kbZnnYh8FflB7vwxJffOY5mngpNplkWxpvEbSFPWJaf+v8Hmz1mYbxFsO1hLV7urZlKQV21v5/IYqvwPukfQdkuMDtb/kZwFrJb1Mcj/mX5OcwfRS+iW/jPxvnfkzkmazZ6Wn0C4kuX9IY34H/F7S7DTzaRHxqaTfAzdJmkfSAun0PDOYNcoN75mZWRbvVjIzsywuDmZmlsXFwczMsrg4mJlZFhcHMzPL4uJgZmZZXBzMzCyLi4OZmWX5/2TDNcuHuXqkAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "N = len(baseline)\n",
    "# N = 5\n",
    "ind = np.arange(N) \n",
    "width = 0.1\n",
    "  \n",
    "vals_taiex = [x * 100 for x in baseline]\n",
    "bar1 = plt.bar(ind, vals_taiex, width, color = 'r')\n",
    "  \n",
    "vals_10 = [(x - 1) * 100 for x in annualized_return[10]]\n",
    "bar2 = plt.bar(ind+width, vals_10, width, color='g')\n",
    "  \n",
    "vals_20 = [(x - 1) * 100 for x in annualized_return[20]]\n",
    "bar3 = plt.bar(ind+width*2, vals_20, width, color = 'm')\n",
    "\n",
    "vals_30 = [(x - 1) * 100 for x in annualized_return[30]]\n",
    "bar4 = plt.bar(ind+width*3, vals_30, width, color = 'darkgray')\n",
    "\n",
    "vals_40 = [(x - 1) * 100 for x in annualized_return[40]]\n",
    "bar5 = plt.bar(ind+width*4, vals_40, width, color = 'darkorange')\n",
    "\n",
    "vals_60 = [(x - 1) * 100 for x in annualized_return[60]]\n",
    "bar6 = plt.bar(ind+width*5, vals_60, width, color = 'y')\n",
    "\n",
    "vals_120 = [(x - 1) * 100 for x in annualized_return[120]]\n",
    "bar7 = plt.bar(ind+width*6, vals_120, width, color = 'c')\n",
    "  \n",
    "plt.xlabel(\"Date Period\")\n",
    "# plt.xlabel(\"Year\")\n",
    "plt.ylabel('Rate of Return (%)')\n",
    "plt.title(\"Annualized Return\")\n",
    "plt.ylim(-15, 27)\n",
    "  \n",
    "plt.xticks(ind+width*2,['2021/\\n01/04\\n~\\n2021/\\n12/27',\n",
    "'2021/\\n02/02\\n~\\n2022/\\n01/26',\n",
    "'2021/\\n03/15\\n~\\n2022/\\n03/08',\n",
    "'2021/\\n04/15\\n~\\n2022/\\n04/08',\n",
    "'2021/\\n05/17\\n~\\n2022/\\n05/10',\n",
    "'2021/\\n06/16\\n~\\n2022/\\n06/09',\n",
    "'2021/\\n07/15\\n~\\n2022/\\n07/08'\n",
    "])\n",
    "\n",
    "plt.legend((bar1, bar2, bar3, bar4, bar5, bar6, bar7),\n",
    "           ('baseline', '10d', '20d', '30d', '40d', '60d', '120d'),\n",
    "           ncol = 2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 8.12\n",
      "10d 8.42\n",
      "20d 8.7\n",
      "30d 9.9\n",
      "40d 10.98\n",
      "60d 14.12\n",
      "120d 4.62\n"
     ]
    }
   ],
   "source": [
    "# Average Annualized Return\n",
    "print('baseline', round(mean(baseline) * 100, 2))\n",
    "print('10d', round((mean(annualized_return[10]) - 1) * 100, 2))\n",
    "print('20d', round((mean(annualized_return[20]) - 1) * 100, 2))\n",
    "print('30d', round((mean(annualized_return[30]) - 1) * 100, 2))\n",
    "print('40d', round((mean(annualized_return[40]) - 1) * 100, 2))\n",
    "print('60d', round((mean(annualized_return[60]) - 1) * 100, 2))\n",
    "print('120d', round((mean(annualized_return[120]) - 1) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('virtual-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e59d1fd077a704e646ff555cb34cfb8f744bf88ce235cb0cbe66c3645bd5281d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
