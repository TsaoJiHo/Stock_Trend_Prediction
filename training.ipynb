{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    ''' Dataset for loading and preprocessing the dataset '''\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 mode='train'\n",
    "              ):\n",
    "        self.mode = mode\n",
    "\n",
    "        # Read data into numpy arrays\n",
    "        with open(path, 'r') as fp:\n",
    "          data = list(csv.reader(fp))\n",
    "          data = np.array(data[1:])[:, 4:].astype(float)\n",
    "        \n",
    "        target = data[:, -1]\n",
    "        data = data[:, list(range(22))]\n",
    "\n",
    "        if mode == 'test':\n",
    "            # Testing data\n",
    "            self.data = torch.FloatTensor(data)\n",
    "            self.target = torch.LongTensor(target)\n",
    "        else:\n",
    "            # Training data (train/dev sets)\n",
    "        \n",
    "            # Splitting training data into train & dev sets\n",
    "            gap = int(len(data) * 0.8)\n",
    "            if mode == 'train':\n",
    "                \n",
    "                self.data = torch.FloatTensor(data[:gap])\n",
    "                self.target = torch.LongTensor(target[:gap])\n",
    "            elif mode == 'dev':\n",
    "                            \n",
    "            # Convert data into PyTorch tensors\n",
    "                self.data = torch.FloatTensor(data[gap:])\n",
    "                self.target = torch.LongTensor(target[gap:])\n",
    "\n",
    "        # Normalize features\n",
    "        self.data = \\\n",
    "            (self.data - self.data.mean(dim=0, keepdim=True)) \\\n",
    "            / self.data.std(dim=0, keepdim=True)\n",
    "\n",
    "        self.dim = self.data.shape[1]\n",
    "\n",
    "        # print(f'Finished reading the {mode} set of Dataset ({len(self.data)} samples found, each dim = {self.dim})')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.data[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(22, 512)\n",
    "        self.layer1_bn=nn.BatchNorm1d(512)\n",
    "\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer2_bn=nn.BatchNorm1d(256)\n",
    "\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer3_bn=nn.BatchNorm1d(128)\n",
    "\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer4_bn=nn.BatchNorm1d(64)\n",
    "\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer5_bn=nn.BatchNorm1d(32)\n",
    "\n",
    "        # self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.out = nn.Linear(32, 3) \n",
    "\n",
    "        # self.act_fn = nn.Sigmoid()\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer1_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer2_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer3_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    " \n",
    "        x = self.layer4(x)\n",
    "        x = self.layer4_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        # x = self.drop(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score \n",
    "plt.ioff()\n",
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "same_seeds(0)\n",
    "\n",
    "# get device \n",
    "device = 'cpu'\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# training parameters\n",
    "num_epoch = 400        # number of training epoch\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.001       # learning rate\n",
    "\n",
    "\n",
    "def macro_precision(y_true, y_predict):\n",
    "    hit_count = {0: 0, 1: 0, 2: 0}\n",
    "    loss_count = 0\n",
    "    for i, prediction in enumerate(y_predict):\n",
    "        if y_true[i] == prediction:\n",
    "            hit_count[prediction] += 1\n",
    "        elif (y_true[i] == 0 and prediction == 2) or (y_true[i] == 2 and prediction == 0):\n",
    "            loss_count += 1\n",
    "    # r1 = hit_count[0]/y_true.count(0)\n",
    "    # r2 = hit_count[1]/y_true.count(1)\n",
    "    # r3 = hit_count[2]/y_true.count(2)\n",
    "\n",
    "    if hit_count[0] == 0:\n",
    "        p1 = 0\n",
    "    else:\n",
    "        p1 = hit_count[0]/y_predict.count(0)\n",
    "    if hit_count[1] == 0:\n",
    "        p2 = 0\n",
    "    else:\n",
    "        p2 = hit_count[1]/y_predict.count(1)\n",
    "    if hit_count[2] == 0:\n",
    "        p3 = 0\n",
    "    else:\n",
    "        p3 = hit_count[2]/y_predict.count(2)\n",
    "\n",
    "    # f1 = 2 * p1 * r1 / (p1 + r1)\n",
    "    # f2 = 2 * p2 * r2 / (p2 + r2)\n",
    "    # f3 = 2 * p3 * r3 / (p3 + r3)\n",
    "    \n",
    "    # return (p1 + p2 + p3) / 3\n",
    "    return hit_count[0] + hit_count[2] - loss_count\n",
    "\n",
    "def plot_learning_curve(acc_record, title, y_label, i , d, limit_y = True):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_epoch = len(acc_record['train'])\n",
    "    x = range(total_epoch)\n",
    "    # x_2 = x_1[::len(acc_record['train']) // len(acc_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x, acc_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x, acc_record['dev'], c='tab:cyan', label='dev')\n",
    "    if limit_y:\n",
    "        plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./models/fig/{y_label}_{i}_{d}d.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3_20d\n",
      "Model 11_10d\n",
      "Model 12_60d\n",
      "Model 19_120d\n",
      "Model 23_10d\n",
      "Model 23_60d\n",
      "Model 25_10d\n",
      "Model 25_20d\n",
      "Model 25_60d\n",
      "Model 25_120d\n",
      "Model 26_20d\n",
      "Model 30_120d\n",
      "Model 31_10d\n",
      "Model 31_20d\n",
      "Model 31_60d\n",
      "Model 31_120d\n",
      "Model 36_20d\n",
      "Model 40_20d\n",
      "Model 41_120d\n",
      "Model 46_20d\n",
      "Model 54_10d\n",
      "Model 54_20d\n",
      "Model 56_60d\n",
      "Model 60_10d\n",
      "Model 71_60d\n",
      "Model 73_20d\n",
      "Model 73_120d\n",
      "Model 77_20d\n",
      "Model 77_120d\n",
      "Model 80_120d\n",
      "Model 90_60d\n",
      "Model 97_10d\n",
      "Model 98_10d\n",
      "Model 98_20d\n",
      "Model 103_60d\n",
      "Model 106_120d\n",
      "Model 113_20d\n",
      "Model 113_60d\n",
      "Model 113_120d\n",
      "Model 116_120d\n",
      "Model 118_10d\n",
      "Model 118_20d\n",
      "Model 118_120d\n",
      "Model 125_10d\n",
      "Model 125_60d\n",
      "Model 130_20d\n",
      "Model 134_20d\n",
      "Model 134_60d\n",
      "Model 139_60d\n",
      "Model 141_20d\n",
      "Model 147_60d\n",
      "Model 153_60d\n",
      "Model 154_60d\n",
      "Model 160_20d\n",
      "Model 160_60d\n",
      "Model 160_120d\n",
      "Model 161_60d\n",
      "Model 161_120d\n",
      "Model 166_60d\n",
      "Model 166_120d\n",
      "Model 169_60d\n",
      "Model 172_60d\n",
      "Model 172_120d\n",
      "Model 178_20d\n",
      "Model 185_20d\n",
      "Model 185_60d\n",
      "Model 185_120d\n",
      "Model 186_20d\n",
      "Model 187_60d\n",
      "Model 187_120d\n",
      "Model 190_60d\n",
      "Model 190_120d\n",
      "Model 191_60d\n",
      "Model 191_120d\n",
      "Model 193_20d\n"
     ]
    }
   ],
   "source": [
    "error_list = []\n",
    "\n",
    "# for i in range(1, 201):\n",
    "#     print(f'Model {i}')\n",
    "#     for d in [10, 20, 60, 120]:\n",
    "        \n",
    "for error in redo_list:\n",
    "        i, d = error\n",
    "        i = int(i)\n",
    "        d = int(d[:-1])\n",
    "        print(f'Model {i}_{d}d')\n",
    "\n",
    "        try:\n",
    "            train_set = StockDataset(f'./data/training_data/training_data_{i}_{d}d.csv', mode = 'test')\n",
    "            val_set = StockDataset(f'./data/training_data/val_data_{i}_{d}d.csv', mode = 'test')\n",
    "            train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) #only shuffle the training data\n",
    "            val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "            # the path where checkpoint saved\n",
    "            model_path = f'./models/models/model_{i}_{d}d.ckpt'\n",
    "\n",
    "            # create model, define a loss function, and optimizer\n",
    "            model = Classifier().to(device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Training\n",
    "\n",
    "            best_acc = 0.0\n",
    "            best_macro_precision = 0.0\n",
    "\n",
    "            acc_record = {'train': [], 'dev': []}\n",
    "            macro_f1_record = {'train': [], 'dev': []}\n",
    "            macro_precision_score_record = {'train': [], 'dev': []}\n",
    "\n",
    "            # init accuracy\n",
    "            train_acc = 0.0\n",
    "            val_acc = 0.0\n",
    "\n",
    "            train_labels = []\n",
    "            train_predictions = []\n",
    "            val_labels = []\n",
    "            val_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, val_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() \n",
    "                    for y in val_pred.cpu().numpy():\n",
    "                        val_predictions.append(y)\n",
    "\n",
    "                    for y in labels.cpu().numpy():\n",
    "                        val_labels.append(y)\n",
    "                \n",
    "                for data in train_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs) \n",
    "                    _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                    train_acc += (train_pred.cpu() == labels.cpu()).sum().item() \n",
    "                    for y in train_pred.cpu().numpy():\n",
    "                        train_predictions.append(y)\n",
    "\n",
    "                    for y in labels.cpu().numpy():\n",
    "                        train_labels.append(y)\n",
    "\n",
    "                acc_record['dev'].append(val_acc/len(val_set))\n",
    "                acc_record['train'].append(train_acc/len(train_set))\n",
    "                macro_f1_record['dev'].append(f1_score(val_labels, val_predictions, average='macro'))\n",
    "                macro_f1_record['train'].append(f1_score(train_labels, train_predictions, average='macro'))\n",
    "                macro_precision_score_record['dev'].append(macro_precision(val_labels, val_predictions))\n",
    "                macro_precision_score_record['train'].append(macro_precision(train_labels, train_predictions))\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            # start training\n",
    "            for epoch in range(num_epoch):\n",
    "            \n",
    "                train_acc = 0.0\n",
    "                train_loss = 0.0\n",
    "                val_acc = 0.0\n",
    "                val_loss = 0.0\n",
    "\n",
    "                train_labels = []\n",
    "                train_predictions = []\n",
    "                val_labels = []\n",
    "                val_predictions = []\n",
    "\n",
    "                # training\n",
    "                model.train() # set the model to training mode\n",
    "                for data in train_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad() \n",
    "                    outputs = model(inputs) \n",
    "                    batch_loss = criterion(outputs, labels)\n",
    "                    _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                    batch_loss.backward() # compute gradient\n",
    "                    optimizer.step() # update model with optimizer\n",
    "\n",
    "                    train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "                    train_loss += batch_loss.item()\n",
    "                    for y in train_pred.cpu().numpy():\n",
    "                        train_predictions.append(y)\n",
    "\n",
    "                    for y in labels.cpu().numpy():\n",
    "                        train_labels.append(y)\n",
    "\n",
    "                \n",
    "                acc_record['train'].append(train_acc/len(train_set))\n",
    "\n",
    "                train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "                macro_f1_record['train'].append(train_f1)\n",
    "\n",
    "                train_weighted_precision = macro_precision(train_labels, train_predictions)\n",
    "                macro_precision_score_record['train'].append(train_weighted_precision)\n",
    "\n",
    "                # validation\n",
    "\n",
    "                model.eval() # set the model to evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    for data in val_loader:\n",
    "                        inputs, labels = data\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        batch_loss = criterion(outputs, labels) \n",
    "                        _, val_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                    \n",
    "                        val_acc += (val_pred.cpu() == labels.cpu()).sum().item() \n",
    "                        val_loss += batch_loss.item()\n",
    "                        for y in val_pred.cpu().numpy():\n",
    "                            val_predictions.append(y)\n",
    "\n",
    "                        for y in labels.cpu().numpy():\n",
    "                            val_labels.append(y)\n",
    "\n",
    "\n",
    "                    acc_record['dev'].append(val_acc/len(val_set))\n",
    "\n",
    "                    val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "                    macro_f1_record['dev'].append(val_f1)\n",
    "\n",
    "                    val_weighted_precision = macro_precision(val_labels, val_predictions)\n",
    "                    macro_precision_score_record['dev'].append(val_weighted_precision)\n",
    "\n",
    "\n",
    "                    # if the model improves, save a checkpoint at this epoch\n",
    "                    if val_weighted_precision > best_macro_precision:\n",
    "                        best_macro_precision = val_weighted_precision\n",
    "                        torch.save(model.state_dict(), model_path)\n",
    "                        # print('[{:03d}/{:03d}] Train Acc: {:3.6f} F1: {:.3f} wP: {:.3f} Loss: {:3.6f} | \\\n",
    "                        # Val Acc: {:3.6f} F1: {:.3f} wP: {:.3f} loss: {:3.6f}'\n",
    "                            # .format(epoch + 1, num_epoch, train_acc/len(train_set), train_f1, train_weighted_precision, train_loss/len(train_loader), \n",
    "                            #                             val_acc/len(val_set), val_f1, val_weighted_precision, val_loss/len(val_loader)))\n",
    "                        # print('saving model with Val Acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "                        # print('saving model with Val Precision {:.3f}'.format(best_macro_precision))\n",
    "\n",
    "\n",
    "            plot_learning_curve(acc_record, 'deep model', 'Accuracy', i, d)\n",
    "            plot_learning_curve(macro_precision_score_record, 'deep model', 'Macro Precision', i, d, limit_y=False)\n",
    "            plot_learning_curve(macro_f1_record, 'deep model', 'Macro F1 Score', i, d)\n",
    "        except Exception as e:\n",
    "            error_list.append([i, d, str(e)])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "    \n",
    "with open(\"errors.json\", \"w\") as outfile:\n",
    "    json.dump(error_list, outfile)\n",
    "with open('errors.json') as jsonfile:\n",
    "    redo_list = json.load(jsonfile)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3', '20d'],\n",
       " ['11', '10d'],\n",
       " ['12', '60d'],\n",
       " ['19', '120d'],\n",
       " ['23', '10d'],\n",
       " ['23', '60d'],\n",
       " ['25', '10d'],\n",
       " ['25', '20d'],\n",
       " ['25', '60d'],\n",
       " ['25', '120d'],\n",
       " ['26', '20d'],\n",
       " ['30', '120d'],\n",
       " ['31', '10d'],\n",
       " ['31', '20d'],\n",
       " ['31', '60d'],\n",
       " ['31', '120d'],\n",
       " ['36', '20d'],\n",
       " ['40', '20d'],\n",
       " ['41', '120d'],\n",
       " ['46', '20d'],\n",
       " ['54', '10d'],\n",
       " ['54', '20d'],\n",
       " ['56', '60d'],\n",
       " ['60', '10d'],\n",
       " ['71', '60d'],\n",
       " ['73', '20d'],\n",
       " ['73', '120d'],\n",
       " ['77', '20d'],\n",
       " ['77', '120d'],\n",
       " ['80', '120d'],\n",
       " ['90', '60d'],\n",
       " ['97', '10d'],\n",
       " ['98', '10d'],\n",
       " ['98', '20d'],\n",
       " ['103', '60d'],\n",
       " ['106', '120d'],\n",
       " ['113', '20d'],\n",
       " ['113', '60d'],\n",
       " ['113', '120d'],\n",
       " ['116', '120d'],\n",
       " ['118', '10d'],\n",
       " ['118', '20d'],\n",
       " ['118', '120d'],\n",
       " ['125', '10d'],\n",
       " ['125', '60d'],\n",
       " ['130', '20d'],\n",
       " ['134', '20d'],\n",
       " ['134', '60d'],\n",
       " ['139', '60d'],\n",
       " ['141', '20d'],\n",
       " ['147', '60d'],\n",
       " ['153', '60d'],\n",
       " ['154', '60d'],\n",
       " ['160', '20d'],\n",
       " ['160', '60d'],\n",
       " ['160', '120d'],\n",
       " ['161', '60d'],\n",
       " ['161', '120d'],\n",
       " ['166', '60d'],\n",
       " ['166', '120d'],\n",
       " ['169', '60d'],\n",
       " ['172', '60d'],\n",
       " ['172', '120d'],\n",
       " ['178', '20d'],\n",
       " ['185', '20d'],\n",
       " ['185', '60d'],\n",
       " ['185', '120d'],\n",
       " ['186', '20d'],\n",
       " ['187', '60d'],\n",
       " ['187', '120d'],\n",
       " ['190', '60d'],\n",
       " ['190', '120d'],\n",
       " ['191', '60d'],\n",
       " ['191', '120d'],\n",
       " ['193', '20d']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "l = []\n",
    "redo_list = []\n",
    "\n",
    "for path in pathlib.Path('./models/models').glob('*.ckpt'):\n",
    "    parts = path.stem.split('_')\n",
    "    l.append([parts[1], parts[2]])\n",
    "\n",
    "for i in range(1, 201):\n",
    "    for d in ['10d', '20d', '60d', '120d']:\n",
    "        model_id = [str((i)), d]\n",
    "        if model_id not in l:\n",
    "            redo_list.append(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3', '20d'],\n",
       " ['11', '10d'],\n",
       " ['12', '60d'],\n",
       " ['19', '120d'],\n",
       " ['23', '10d'],\n",
       " ['23', '60d'],\n",
       " ['25', '10d'],\n",
       " ['25', '20d'],\n",
       " ['25', '60d'],\n",
       " ['25', '120d'],\n",
       " ['26', '20d'],\n",
       " ['30', '120d'],\n",
       " ['31', '10d'],\n",
       " ['31', '20d'],\n",
       " ['31', '60d'],\n",
       " ['31', '120d'],\n",
       " ['36', '20d'],\n",
       " ['40', '20d'],\n",
       " ['41', '120d'],\n",
       " ['46', '20d'],\n",
       " ['54', '10d'],\n",
       " ['54', '20d'],\n",
       " ['56', '60d'],\n",
       " ['60', '10d'],\n",
       " ['71', '60d'],\n",
       " ['73', '20d'],\n",
       " ['73', '120d'],\n",
       " ['77', '20d'],\n",
       " ['77', '120d'],\n",
       " ['80', '120d'],\n",
       " ['90', '60d'],\n",
       " ['97', '10d'],\n",
       " ['98', '10d'],\n",
       " ['98', '20d'],\n",
       " ['103', '60d'],\n",
       " ['106', '120d'],\n",
       " ['113', '20d'],\n",
       " ['113', '60d'],\n",
       " ['113', '120d'],\n",
       " ['116', '120d'],\n",
       " ['118', '10d'],\n",
       " ['118', '20d'],\n",
       " ['118', '120d'],\n",
       " ['125', '10d'],\n",
       " ['125', '60d'],\n",
       " ['130', '20d'],\n",
       " ['134', '20d'],\n",
       " ['134', '60d'],\n",
       " ['139', '60d'],\n",
       " ['141', '20d'],\n",
       " ['147', '60d'],\n",
       " ['153', '60d'],\n",
       " ['154', '60d'],\n",
       " ['160', '20d'],\n",
       " ['160', '60d'],\n",
       " ['160', '120d'],\n",
       " ['161', '60d'],\n",
       " ['161', '120d'],\n",
       " ['166', '60d'],\n",
       " ['166', '120d'],\n",
       " ['169', '60d'],\n",
       " ['172', '60d'],\n",
       " ['172', '120d'],\n",
       " ['178', '20d'],\n",
       " ['185', '20d'],\n",
       " ['185', '60d'],\n",
       " ['185', '120d'],\n",
       " ['186', '20d'],\n",
       " ['187', '60d'],\n",
       " ['187', '120d'],\n",
       " ['190', '60d'],\n",
       " ['190', '120d'],\n",
       " ['191', '60d'],\n",
       " ['191', '120d'],\n",
       " ['193', '20d']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          model,\n",
    "                          d,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    # plt.xlabel(f'Predicted label\\naccuracy={accuracy}')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f'./models/performance/model_{model}_{d}d.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 10\n",
      "model 20\n",
      "model 30\n",
      "model 40\n",
      "model 50\n",
      "model 60\n",
      "model 70\n",
      "model 80\n",
      "model 90\n",
      "model 100\n",
      "model 110\n",
      "model 120\n",
      "model 130\n",
      "model 140\n",
      "model 150\n",
      "model 160\n",
      "model 170\n",
      "model 180\n",
      "model 190\n",
      "model 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "\n",
    "with open('performance.csv', 'w', newline='') as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(['model', 'd', 'acc', 'r1', 'r2', 'r3', 'p1', 'p2', 'p3', 'f1', 'f2', 'f3'])\n",
    "\n",
    "with open('trend_params.json') as jsonfile:\n",
    "    code_para_map = json.load(jsonfile)\n",
    "\n",
    "with open('codes.txt', 'r') as f:\n",
    "    codes = f.read().split()\n",
    "\n",
    "ROI = {10: 0, 20: 0, 60:0, 120:0}\n",
    "\n",
    "for i in range(1, 201):\n",
    "  if i % 10 == 0:\n",
    "    print(f'model {i}')\n",
    "  for d in [10, 20, 60, 120]:\n",
    "    \n",
    "    model_path = f'./models/models/model_{i}_{d}d.ckpt'\n",
    "    # create testing dataset\n",
    "    test_set = StockDataset(f'./data/training_data/val_data_{i}_{d}d.csv', mode = 'test')\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "    # create model and load weights from checkpoint\n",
    "    model = Classifier().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "      test_acc = 0.0\n",
    "      predict = []\n",
    "      label = []\n",
    "      for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs) \n",
    "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "\n",
    "        test_acc += (test_pred.cpu() == labels.cpu()).sum().item() \n",
    "\n",
    "        for y in test_pred.cpu().numpy():\n",
    "          predict.append(y)\n",
    "\n",
    "        for y in labels.cpu().numpy():\n",
    "          label.append(y)\n",
    "\n",
    "    hit_count = {0: 0, 1: 0, 2: 0}\n",
    "    loss_count = 0\n",
    "    for j, prediction in enumerate(predict):\n",
    "      if label[j] == prediction:\n",
    "        hit_count[prediction] += 1\n",
    "      elif (label[j] == 0 and prediction == 2) or (label[j] == 2 and prediction == 0):\n",
    "        loss_count += 1\n",
    "   \n",
    "    acc = (hit_count[0] + hit_count[1] + hit_count[2]) / (label.count(0) + label.count(1) + label.count(2))\n",
    "    acc = round(acc, 4)\n",
    "    \n",
    "    r1 = round(hit_count[0]/label.count(0), 4)\n",
    "    r2 = round(hit_count[1]/label.count(1), 4)\n",
    "    r3 = round(hit_count[2]/label.count(2), 4)\n",
    "\n",
    "    if predict.count(0) == 0:\n",
    "      p1 = 0\n",
    "    else:\n",
    "      p1 = round(hit_count[0]/predict.count(0), 4)\n",
    "\n",
    "    if predict.count(1) == 0:\n",
    "      p2 = 0\n",
    "    else:\n",
    "      p2 = round(hit_count[1]/predict.count(1), 4)\n",
    "\n",
    "    if predict.count(2) == 0:\n",
    "      p3 = 0\n",
    "    else:\n",
    "      p3 = round(hit_count[2]/predict.count(2), 4)\n",
    "\n",
    "    if hit_count[0] == 0:\n",
    "      f1 = 0\n",
    "    else:\n",
    "      f1 = round(2 * p1 * r1 / (p1 + r1), 4)\n",
    "\n",
    "    if hit_count[1] == 0:\n",
    "      f2 = 0\n",
    "    else:\n",
    "      f2 = round(2 * p2 * r2 / (p2 + r2), 4)\n",
    "\n",
    "    if hit_count[2] == 0:\n",
    "      f3 = 0\n",
    "    else:\n",
    "      f3 = round(2 * p3 * r3 / (p3 + r3), 4)\n",
    "\n",
    "    with open('performance.csv', 'a', newline='') as csvfile:\n",
    "      writer = csv.writer(csvfile)\n",
    "      writer.writerow([i, d, acc, r1, r2, r3, p1, p2, p3, f1, f2, f3])\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(label, predict),\n",
    "                          ['Down', 'Stable' ,'Up'],\n",
    "                          i,\n",
    "                          d,\n",
    "                          normalize    = False\n",
    "                         )\n",
    "    gain = code_para_map[codes[i-1]][f'{d}d'][0] - 1\n",
    "    ROI[d] += gain * (hit_count[0] + hit_count[2] - loss_count) / len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.8666353317275792,\n",
       " 20: 1.565148117865777,\n",
       " 60: 4.1748055613749795,\n",
       " 120: 8.56690323132078}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.004333176658637896,\n",
       " 20: 0.007825740589328884,\n",
       " 60: 0.020874027806874897,\n",
       " 120: 0.0428345161566039}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in ROI:\n",
    "    ROI[d] /= 200\n",
    "ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15: 0.16074503155365555,\n",
       " 30: 0.1415036395974974,\n",
       " 90: 0.9053511935807522,\n",
       " 180: 1.5318047851025869}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15: 0.0008037251577682778,\n",
       " 30: 0.0007075181979874869,\n",
       " 90: 0.004526755967903761,\n",
       " 180: 0.007659023925512935}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in [15, 30, 90, 180]:\n",
    "    ROI[d] /= 200\n",
    "\n",
    "ROI"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bad007a486731be1b72cebf8069ca022872976a844f0a9d919fbbe79314a6909"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('thesis-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
