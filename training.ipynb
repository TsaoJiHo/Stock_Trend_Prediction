{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    ''' Dataset for loading and preprocessing the dataset '''\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 mode='train'\n",
    "              ):\n",
    "        self.mode = mode\n",
    "\n",
    "        # Read data into numpy arrays\n",
    "        with open(path, 'r') as fp:\n",
    "            data = list(csv.reader(fp))\n",
    "            data = np.array(data[1:])[:, 4:].astype(float)\n",
    "        \n",
    "        \n",
    "\n",
    "        target = data[:, -1]\n",
    "        data = data[:, list(range(22))]\n",
    "\n",
    "        \n",
    "\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.target = torch.LongTensor(target)\n",
    "\n",
    "\n",
    "        # Normalize features\n",
    "        self.data = \\\n",
    "            (self.data - self.data.mean(dim=0, keepdim=True)) \\\n",
    "            / self.data.std(dim=0, keepdim=True)\n",
    "\n",
    "        self.dim = self.data.shape[1]\n",
    "\n",
    "        # print(f'Finished reading the {mode} set of Dataset ({len(self.data)} samples found, each dim = {self.dim})')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.data[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layer2 = nn.Linear(22, 256)\n",
    "        self.layer2_bn=nn.BatchNorm1d(256)\n",
    "\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer3_bn=nn.BatchNorm1d(128)\n",
    "\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer4_bn=nn.BatchNorm1d(64)\n",
    "\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer5_bn=nn.BatchNorm1d(32)\n",
    "\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "        self.out = nn.Linear(32, 3) \n",
    "        \n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer2_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer3_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.drop(x)\n",
    " \n",
    "        x = self.layer4(x)\n",
    "        x = self.layer4_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score \n",
    "plt.ioff()\n",
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "same_seeds(0)\n",
    "\n",
    "# get device \n",
    "device = 'cpu'\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# training parameters\n",
    "num_epoch = 100        # number of training epoch\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.001       # learning rate\n",
    "\n",
    "\n",
    "def macro_precision(y_true, y_predict):\n",
    "    hit_count = {0: 0, 1: 0, 2: 0}\n",
    "    loss_count = 0\n",
    "    for i, prediction in enumerate(y_predict):\n",
    "        if y_true[i] == prediction:\n",
    "            hit_count[prediction] += 1\n",
    "        elif (y_true[i] == 0 and prediction == 2) or (y_true[i] == 2 and prediction == 0):\n",
    "            loss_count += 1\n",
    "    # r1 = hit_count[0]/y_true.count(0)\n",
    "    # r2 = hit_count[1]/y_true.count(1)\n",
    "    # r3 = hit_count[2]/y_true.count(2)\n",
    "\n",
    "    if hit_count[0] == 0:\n",
    "        p1 = 0\n",
    "    else:\n",
    "        p1 = hit_count[0]/y_predict.count(0)\n",
    "    if hit_count[1] == 0:\n",
    "        p2 = 0\n",
    "    else:\n",
    "        p2 = hit_count[1]/y_predict.count(1)\n",
    "    if hit_count[2] == 0:\n",
    "        p3 = 0\n",
    "    else:\n",
    "        p3 = hit_count[2]/y_predict.count(2)\n",
    "\n",
    "    # f1 = 2 * p1 * r1 / (p1 + r1)\n",
    "    # f2 = 2 * p2 * r2 / (p2 + r2)\n",
    "    # f3 = 2 * p3 * r3 / (p3 + r3)\n",
    "    \n",
    "    # return (p1 + p2 + p3) / 3\n",
    "    return hit_count[0] + hit_count[2] - loss_count\n",
    "\n",
    "def plot_learning_curve(acc_record, title, y_label, i , d, limit_y = True):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_epoch = len(acc_record['train'])\n",
    "    x = range(total_epoch)\n",
    "    # x_2 = x_1[::len(acc_record['train']) // len(acc_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x, acc_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x, acc_record['dev'], c='tab:cyan', label='dev')\n",
    "    if limit_y:\n",
    "        plt.ylim(0.0, 1.0)\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./models_300/fig/{y_label}_{i}_{d}d.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Model 2\n",
      "Model 3\n",
      "Model 4\n",
      "Model 5\n",
      "Model 6\n",
      "Model 7\n",
      "Model 8\n",
      "Model 9\n",
      "Model 10\n",
      "Model 11\n",
      "Model 12\n",
      "Model 13\n",
      "Model 14\n",
      "Model 15\n",
      "Model 16\n",
      "Model 17\n",
      "Model 18\n",
      "Model 19\n",
      "Model 20\n",
      "Model 21\n",
      "Model 22\n",
      "Model 23\n",
      "Model 24\n",
      "Model 25\n",
      "Model 26\n",
      "Model 27\n",
      "Model 28\n",
      "Model 29\n",
      "Model 30\n",
      "Model 31\n",
      "Model 32\n",
      "Model 33\n",
      "Model 34\n",
      "Model 35\n",
      "Model 36\n",
      "Model 37\n",
      "Model 38\n",
      "Model 39\n",
      "Model 40\n",
      "Model 41\n",
      "Model 42\n",
      "Model 43\n",
      "Model 44\n",
      "Model 45\n",
      "Model 46\n",
      "Model 47\n",
      "Model 48\n",
      "Model 49\n",
      "Model 50\n",
      "Model 51\n",
      "Model 52\n",
      "Model 53\n",
      "Model 54\n",
      "Model 55\n",
      "Model 56\n",
      "Model 57\n",
      "Model 58\n",
      "Model 59\n",
      "Model 60\n",
      "Model 61\n",
      "Model 62\n",
      "Model 63\n",
      "Model 64\n",
      "Model 65\n",
      "Model 66\n",
      "Model 67\n",
      "Model 68\n",
      "Model 69\n",
      "Model 70\n",
      "Model 71\n",
      "Model 72\n",
      "Model 73\n",
      "Model 74\n",
      "Model 75\n",
      "Model 76\n",
      "Model 77\n",
      "Model 78\n",
      "Model 79\n",
      "Model 80\n",
      "Model 81\n",
      "Model 82\n",
      "Model 83\n",
      "Model 84\n",
      "Model 85\n",
      "Model 86\n",
      "Model 87\n",
      "Model 88\n",
      "Model 89\n",
      "Model 90\n",
      "Model 91\n",
      "Model 92\n",
      "Model 93\n",
      "Model 94\n",
      "Model 95\n",
      "Model 96\n",
      "Model 97\n",
      "Model 98\n",
      "Model 99\n",
      "Model 100\n",
      "Model 101\n",
      "Model 102\n",
      "Model 103\n",
      "Model 104\n",
      "Model 105\n",
      "Model 106\n",
      "Model 107\n",
      "Model 108\n",
      "Model 109\n",
      "Model 110\n",
      "Model 111\n",
      "Model 112\n",
      "Model 113\n",
      "Model 114\n",
      "Model 115\n",
      "Model 116\n",
      "Model 117\n",
      "Model 118\n",
      "Model 119\n",
      "Model 120\n",
      "Model 121\n",
      "Model 122\n",
      "Model 123\n",
      "Model 124\n",
      "Model 125\n",
      "Model 126\n",
      "Model 127\n",
      "Model 128\n",
      "Model 129\n",
      "Model 130\n",
      "Model 131\n",
      "Model 132\n",
      "Model 133\n",
      "Model 134\n",
      "Model 135\n",
      "Model 136\n",
      "Model 137\n",
      "Model 138\n",
      "Model 139\n",
      "Model 140\n",
      "Model 141\n",
      "Model 142\n",
      "Model 143\n",
      "Model 144\n",
      "Model 145\n",
      "Model 146\n",
      "Model 147\n",
      "Model 148\n",
      "Model 149\n",
      "Model 150\n",
      "Model 151\n",
      "Model 152\n",
      "Model 153\n",
      "Model 154\n",
      "Model 155\n",
      "Model 156\n",
      "Model 157\n",
      "Model 158\n",
      "Model 159\n",
      "Model 160\n",
      "Model 161\n",
      "Model 162\n",
      "Model 163\n",
      "Model 164\n",
      "Model 165\n",
      "Model 166\n",
      "Model 167\n",
      "Model 168\n",
      "Model 169\n",
      "Model 170\n",
      "Model 171\n",
      "Model 172\n",
      "Model 173\n",
      "Model 174\n",
      "Model 175\n",
      "Model 176\n",
      "Model 177\n",
      "Model 178\n",
      "Model 179\n",
      "Model 180\n",
      "Model 181\n",
      "Model 182\n",
      "Model 183\n",
      "Model 184\n",
      "Model 185\n",
      "Model 186\n",
      "Model 187\n",
      "Model 188\n",
      "Model 189\n",
      "Model 190\n",
      "Model 191\n",
      "Model 192\n",
      "Model 193\n",
      "Model 194\n",
      "Model 195\n",
      "Model 196\n",
      "Model 197\n",
      "Model 198\n",
      "Model 199\n",
      "Model 200\n",
      "Model 201\n",
      "Model 202\n",
      "Model 203\n",
      "Model 204\n",
      "Model 205\n",
      "Model 206\n",
      "Model 207\n",
      "Model 208\n",
      "Model 209\n",
      "Model 210\n",
      "Model 211\n",
      "Model 212\n",
      "Model 213\n",
      "Model 214\n",
      "Model 215\n",
      "Model 216\n",
      "Model 217\n",
      "Model 218\n",
      "Model 219\n",
      "Model 220\n",
      "Model 221\n",
      "Model 222\n",
      "Model 223\n",
      "Model 224\n",
      "Model 225\n",
      "Model 226\n",
      "Model 227\n",
      "Model 228\n",
      "Model 229\n",
      "Model 230\n",
      "Model 231\n",
      "Model 232\n",
      "Model 233\n",
      "Model 234\n",
      "Model 235\n",
      "Model 236\n",
      "Model 237\n",
      "Model 238\n",
      "Model 239\n",
      "Model 240\n",
      "Model 241\n",
      "Model 242\n",
      "Model 243\n",
      "Model 244\n",
      "Model 245\n",
      "Model 246\n",
      "Model 247\n",
      "Model 248\n",
      "Model 249\n",
      "Model 250\n",
      "Model 251\n",
      "Model 252\n",
      "Model 253\n",
      "Model 254\n",
      "Model 255\n",
      "Model 256\n",
      "Model 257\n",
      "Model 258\n",
      "Model 259\n",
      "Model 260\n",
      "Model 261\n",
      "Model 262\n",
      "Model 263\n",
      "Model 264\n",
      "Model 265\n",
      "Model 266\n",
      "Model 267\n",
      "Model 268\n",
      "Model 269\n",
      "Model 270\n",
      "Model 271\n",
      "Model 272\n",
      "Model 273\n",
      "Model 274\n",
      "Model 275\n",
      "Model 276\n",
      "Model 277\n",
      "Model 278\n",
      "Model 279\n",
      "Model 280\n",
      "Model 281\n",
      "Model 282\n",
      "Model 283\n",
      "Model 284\n",
      "Model 285\n",
      "Model 286\n",
      "Model 287\n",
      "Model 288\n",
      "Model 289\n",
      "Model 290\n",
      "Model 291\n",
      "Model 292\n",
      "Model 293\n",
      "Model 294\n",
      "Model 295\n",
      "Model 296\n",
      "Model 297\n",
      "Model 298\n",
      "Model 299\n",
      "Model 300\n",
      "Model 301\n",
      "Model 302\n",
      "Model 303\n",
      "Model 304\n",
      "Model 305\n",
      "Model 306\n",
      "Model 307\n",
      "Model 308\n",
      "Model 309\n",
      "Model 310\n",
      "Model 311\n",
      "Model 312\n",
      "Model 313\n",
      "Model 314\n",
      "Model 315\n",
      "Model 316\n",
      "Model 317\n",
      "Model 318\n",
      "Model 319\n",
      "Model 320\n",
      "Model 321\n",
      "Model 322\n",
      "Model 323\n",
      "Model 324\n",
      "Model 325\n",
      "Model 326\n",
      "Model 327\n",
      "Model 328\n",
      "Model 329\n",
      "Model 330\n",
      "Model 331\n",
      "Model 332\n",
      "Model 333\n",
      "Model 334\n",
      "Model 335\n",
      "Model 336\n",
      "Model 337\n",
      "Model 338\n",
      "Model 339\n",
      "Model 340\n",
      "Model 341\n",
      "Model 342\n",
      "Model 343\n",
      "Model 344\n",
      "Model 345\n",
      "Model 346\n",
      "Model 347\n",
      "Model 348\n",
      "Model 349\n",
      "Model 350\n",
      "Model 351\n",
      "Model 352\n",
      "Model 353\n",
      "Model 354\n",
      "Model 355\n"
     ]
    }
   ],
   "source": [
    "error_list = []\n",
    "\n",
    "for i in range(1, 356):\n",
    "    print(f'Model {i}')\n",
    "    for d in [10, 20, 30, 40, 60, 120]:\n",
    "       \n",
    "# for error in redo_list:\n",
    "#         i, d = error\n",
    "#         i = int(i)\n",
    "#         d = int(d[:-1])\n",
    "#         print(f'Model {i}_{d}d')\n",
    "        early_stop_count = 0\n",
    "        \n",
    "        train_set = StockDataset(f'./data/training_data_300/training_data_{i}_{d}d.csv')\n",
    "        val_set = StockDataset(f'./data/training_data_300/testing_data_{i}_{d}d.csv')\n",
    "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) #only shuffle the training data\n",
    "        val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # the path where checkpoint saved\n",
    "        model_path = f'./models_300/models/model_{i}_{d}d.ckpt'\n",
    "\n",
    "        # create model, define a loss function, and optimizer\n",
    "        model = Classifier().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training\n",
    "\n",
    "        best_acc = 0.0\n",
    "        best_macro_precision = 0.0\n",
    "\n",
    "        acc_record = {'train': [], 'dev': []}\n",
    "        macro_f1_record = {'train': [], 'dev': []}\n",
    "        macro_precision_score_record = {'train': [], 'dev': []}\n",
    "\n",
    "        # init accuracy\n",
    "        train_acc = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        train_labels = []\n",
    "        train_predictions = []\n",
    "        val_labels = []\n",
    "        val_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, val_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() \n",
    "                for y in val_pred.cpu().numpy():\n",
    "                    val_predictions.append(y)\n",
    "\n",
    "                for y in labels.cpu().numpy():\n",
    "                    val_labels.append(y)\n",
    "            \n",
    "            for data in train_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs) \n",
    "                _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                train_acc += (train_pred.cpu() == labels.cpu()).sum().item() \n",
    "                for y in train_pred.cpu().numpy():\n",
    "                    train_predictions.append(y)\n",
    "\n",
    "                for y in labels.cpu().numpy():\n",
    "                    train_labels.append(y)\n",
    "\n",
    "            acc_record['dev'].append(val_acc/len(val_set))\n",
    "            acc_record['train'].append(train_acc/len(train_set))\n",
    "            macro_f1_record['dev'].append(f1_score(val_labels, val_predictions, average='macro'))\n",
    "            macro_f1_record['train'].append(f1_score(train_labels, train_predictions, average='macro'))\n",
    "            macro_precision_score_record['dev'].append(macro_precision(val_labels, val_predictions))\n",
    "            macro_precision_score_record['train'].append(macro_precision(train_labels, train_predictions))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # start training\n",
    "        for epoch in range(num_epoch):\n",
    "        \n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            train_labels = []\n",
    "            train_predictions = []\n",
    "            val_labels = []\n",
    "            val_predictions = []\n",
    "\n",
    "            # training\n",
    "            model.train() # set the model to training mode\n",
    "            for data in train_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad() \n",
    "                outputs = model(inputs) \n",
    "                batch_loss = criterion(outputs, labels)\n",
    "                _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                batch_loss.backward() # compute gradient\n",
    "                optimizer.step() # update model with optimizer\n",
    "\n",
    "                train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "                train_loss += batch_loss.item()\n",
    "                for y in train_pred.cpu().numpy():\n",
    "                    train_predictions.append(y)\n",
    "\n",
    "                for y in labels.cpu().numpy():\n",
    "                    train_labels.append(y)\n",
    "\n",
    "            \n",
    "            acc_record['train'].append(train_acc/len(train_set))\n",
    "\n",
    "            train_f1 = f1_score(train_labels, train_predictions, average='macro')\n",
    "            macro_f1_record['train'].append(train_f1)\n",
    "\n",
    "            train_weighted_precision = macro_precision(train_labels, train_predictions)\n",
    "            macro_precision_score_record['train'].append(train_weighted_precision)\n",
    "\n",
    "            # validation\n",
    "\n",
    "            model.eval() # set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    batch_loss = criterion(outputs, labels) \n",
    "                    _, val_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "                \n",
    "                    val_acc += (val_pred.cpu() == labels.cpu()).sum().item() \n",
    "                    val_loss += batch_loss.item()\n",
    "                    for y in val_pred.cpu().numpy():\n",
    "                        val_predictions.append(y)\n",
    "\n",
    "                    for y in labels.cpu().numpy():\n",
    "                        val_labels.append(y)\n",
    "\n",
    "\n",
    "                acc_record['dev'].append(val_acc/len(val_set))\n",
    "\n",
    "                val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "                macro_f1_record['dev'].append(val_f1)\n",
    "\n",
    "                val_weighted_precision = macro_precision(val_labels, val_predictions)\n",
    "                macro_precision_score_record['dev'].append(val_weighted_precision)\n",
    "\n",
    "\n",
    "                # if the model improves, save a checkpoint at this epoch\n",
    "                if val_weighted_precision > best_macro_precision:\n",
    "                    best_macro_precision = val_weighted_precision\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    # print('[{:03d}/{:03d}] Train Acc: {:3.6f} F1: {:.3f} wP: {:.3f} Loss: {:3.6f} | \\\n",
    "                    # Val Acc: {:3.6f} F1: {:.3f} wP: {:.3f} loss: {:3.6f}'\n",
    "                        # .format(epoch + 1, num_epoch, train_acc/len(train_set), train_f1, train_weighted_precision, train_loss/len(train_loader), \n",
    "                        #                             val_acc/len(val_set), val_f1, val_weighted_precision, val_loss/len(val_loader)))\n",
    "                    # print('saving model with Val Acc {:.3f}'.format(best_acc/len(val_set)))\n",
    "                    # print('saving model with Val Precision {:.3f}'.format(best_macro_precision))\n",
    "                    early_stop_count = 0\n",
    "                else:\n",
    "                    early_stop_count += 1\n",
    "        \n",
    "            # if early_stop_count >= 20:\n",
    "            #     break\n",
    "\n",
    "        plot_learning_curve(acc_record, 'deep model', 'Accuracy', i, d)\n",
    "        plot_learning_curve(macro_precision_score_record, 'deep model', 'Macro Precision', i, d, limit_y=False)\n",
    "        plot_learning_curve(macro_f1_record, 'deep model', 'Macro F1 Score', i, d)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "    \n",
    "with open(\"errors.json\", \"w\") as outfile:\n",
    "    json.dump(error_list, outfile)\n",
    "with open('errors.json') as jsonfile:\n",
    "    redo_list = json.load(jsonfile)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          model,\n",
    "                          d,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    # plt.xlabel(f'Predicted label\\naccuracy={accuracy}')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f'./models_300/performance/model_{model}_{d}d.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average annualized return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "import json\n",
    "\n",
    "with open('performance_300.csv', 'w', newline='') as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(['model', 'd', 'acc', 'r1', 'r2', 'r3', 'p1', 'p2', 'p3', 'f1', 'f2', 'f3'])\n",
    "\n",
    "with open('trend_params_300.json') as jsonfile:\n",
    "    code_para_map = json.load(jsonfile)\n",
    "\n",
    "with open('codes_300.txt', 'r') as f:\n",
    "    codes = f.read().split()\n",
    "\n",
    "ROI = {10: 0, 20: 0, 30: 0, 40: 0, 60: 0, 120: 0}\n",
    "predicted_label = {10: [], 20: [], 30: [], 40: [], 60: [], 120: []}\n",
    "true_label = {10: [], 20: [], 30: [], 40: [], 60: [], 120: []}\n",
    "\n",
    "for i in range(1, 356):\n",
    "    \n",
    "  for d in [10, 20, 30, 40, 60, 120]:\n",
    "    # print(d)\n",
    "    model_path = f'./models_300/models/model_{i}_{d}d.ckpt'\n",
    "    # create testing dataset\n",
    "    test_set = StockDataset(f'./data/training_data_300/testing_data_{i}_{d}d.csv')\n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # create model and load weights from checkpoint\n",
    "    model = Classifier().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "      test_acc = 0.0\n",
    "      predict = []\n",
    "      label = []\n",
    "      for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs) \n",
    "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "\n",
    "        test_acc += (test_pred.cpu() == labels.cpu()).sum().item() \n",
    "\n",
    "        for y in test_pred.cpu().numpy():\n",
    "          predict.append(y)\n",
    "          predicted_label[d].append(y)\n",
    "\n",
    "        for y in labels.cpu().numpy():\n",
    "          label.append(y)\n",
    "          true_label[d].append(y)\n",
    "\n",
    "    hit_count = {0: 0, 1: 0, 2: 0}\n",
    "    loss_count = 0\n",
    "    for j, prediction in enumerate(predict):\n",
    "      if label[j] == prediction:\n",
    "        hit_count[prediction] += 1\n",
    "      elif (label[j] == 0 and prediction == 2) or (label[j] == 2 and prediction == 0):\n",
    "        loss_count += 1\n",
    "   \n",
    "    acc = (hit_count[0] + hit_count[1] + hit_count[2]) / (label.count(0) + label.count(1) + label.count(2))\n",
    "    acc = round(acc, 4)\n",
    "    \n",
    "    if label.count(0) == 0:\n",
    "      r1 = 0\n",
    "    else:\n",
    "      r1 = round(hit_count[0]/label.count(0), 4)\n",
    "\n",
    "    if label.count(1) == 0:\n",
    "      r2 = 0\n",
    "    else:\n",
    "      r2 = round(hit_count[1]/label.count(1), 4)\n",
    "\n",
    "    if label.count(2) == 0:\n",
    "      r3 = 0\n",
    "    else:\n",
    "      r3 = round(hit_count[2]/label.count(2), 4)\n",
    "\n",
    "    if predict.count(0) == 0:\n",
    "      p1 = 0\n",
    "    else:\n",
    "      p1 = round(hit_count[0]/predict.count(0), 4)\n",
    "\n",
    "    if predict.count(1) == 0:\n",
    "      p2 = 0\n",
    "    else:\n",
    "      p2 = round(hit_count[1]/predict.count(1), 4)\n",
    "\n",
    "    if predict.count(2) == 0:\n",
    "      p3 = 0\n",
    "    else:\n",
    "      p3 = round(hit_count[2]/predict.count(2), 4)\n",
    "\n",
    "    if hit_count[0] == 0:\n",
    "      f1 = 0\n",
    "    else:\n",
    "      f1 = round(2 * p1 * r1 / (p1 + r1), 4)\n",
    "\n",
    "    if hit_count[1] == 0:\n",
    "      f2 = 0\n",
    "    else:\n",
    "      f2 = round(2 * p2 * r2 / (p2 + r2), 4)\n",
    "\n",
    "    if hit_count[2] == 0:\n",
    "      f3 = 0\n",
    "    else:\n",
    "      f3 = round(2 * p3 * r3 / (p3 + r3), 4)\n",
    "\n",
    "    with open('performance_300.csv', 'a', newline='') as csvfile:\n",
    "      writer = csv.writer(csvfile)\n",
    "      writer.writerow([i, d, acc, r1, r2, r3, p1, p2, p3, f1, f2, f3])\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(label, predict),\n",
    "                          ['Down', 'Stable' ,'Up'],\n",
    "                          i,\n",
    "                          d,\n",
    "                          normalize    = False\n",
    "                         )\n",
    "    fluctuation = code_para_map[codes[i-1]][f'{d}d'][0] - 1\n",
    "    ROI[d] += fluctuation * (hit_count[0] + hit_count[2] - loss_count) / len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix of 200 models combination\n",
    "for d in [10, 20, 30, 40, 60, 120]:\n",
    "    plot_confusion_matrix(confusion_matrix(true_label[d], predicted_label[d]),\n",
    "                            ['Down', 'Stable' ,'Up'],\n",
    "                            'total',\n",
    "                            d,\n",
    "                            title=f'Confusion matrix of {d}d',\n",
    "                            normalize    = False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.005029376257545268,\n",
       " 20: 0.01061899792195798,\n",
       " 30: 0.016045586058725247,\n",
       " 40: 0.02045522115147022,\n",
       " 60: 0.028927823545043856,\n",
       " 120: 0.043964891789763004}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average return\n",
    "for d in ROI:\n",
    "    ROI[d] /= 355\n",
    "ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10d 12.8\n",
      "20d 13.51\n",
      "30d 13.58\n",
      "40d 12.92\n",
      "60d 12.08\n",
      "120d 8.99\n"
     ]
    }
   ],
   "source": [
    "# Average annualized return\n",
    "print('10d', round((((ROI[10]+1)**24)-1)*100, 2))\n",
    "print('20d', round((((ROI[20]+1)**12)-1)*100, 2))\n",
    "print('30d', round((((ROI[30]+1)**8)-1)*100, 2))\n",
    "print('40d', round((((ROI[40]+1)**6)-1)*100, 2))\n",
    "print('60d', round((((ROI[60]+1)**4)-1)*100, 2))\n",
    "print('120d', round((((ROI[120]+1)**2)-1)*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10d 2.99\n",
      "20d 8.5\n",
      "30d 10.23\n",
      "40d 10.42\n",
      "60d 10.44\n",
      "120d 8.19\n"
     ]
    }
   ],
   "source": [
    "# Average annualized return after tax\n",
    "print('10d', round((((ROI[10]+1-0.0038)**24)-1)*100, 2))\n",
    "print('20d', round((((ROI[20]+1-0.0038)**12)-1)*100, 2))\n",
    "print('30d', round((((ROI[30]+1-0.0038)**8)-1)*100, 2))\n",
    "print('40d', round((((ROI[40]+1-0.0038)**6)-1)*100, 2))\n",
    "print('60d', round((((ROI[60]+1-0.0038)**4)-1)*100, 2))\n",
    "print('120d', round((((ROI[120]+1-0.0038)**2)-1)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average annualized return of top 20 stocks selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "\n",
    "with open('trend_params.json') as jsonfile:\n",
    "    code_para_map = json.load(jsonfile)\n",
    "\n",
    "with open('codes.txt', 'r') as f:\n",
    "    codes = f.read().split()\n",
    "\n",
    "\n",
    "# predicted_label = {10: [[] for _ in range(315)], 20: [[] for _ in range(305)],\n",
    "#                    30: [[] for _ in range(295)], 40: [[] for _ in range(285)],\n",
    "#                    60: [[] for _ in range(265)], 120:[[] for _ in range(205)]}\n",
    "                  \n",
    "# predicted_value = {10: [[] for _ in range(315)], 20: [[] for _ in range(305)],\n",
    "#                    30: [[] for _ in range(295)], 40: [[] for _ in range(285)],\n",
    "#                    60: [[] for _ in range(265)], 120:[[] for _ in range(205)]}\n",
    "\n",
    "# true_label =      {10: [[] for _ in range(315)], 20: [[] for _ in range(305)],\n",
    "#                    30: [[] for _ in range(295)], 40: [[] for _ in range(285)],\n",
    "#                    60: [[] for _ in range(265)], 120:[[] for _ in range(205)]}\n",
    "\n",
    "max_sample_count = 4000\n",
    "\n",
    "predicted_label = {10: [[] for _ in range(max_sample_count)], 20: [[] for _ in range(max_sample_count)],\n",
    "                   30: [[] for _ in range(max_sample_count)], 40: [[] for _ in range(max_sample_count)],\n",
    "                   60: [[] for _ in range(max_sample_count)], 120:[[] for _ in range(max_sample_count)]}\n",
    "                  \n",
    "predicted_value = {10: [[] for _ in range(max_sample_count)], 20: [[] for _ in range(max_sample_count)],\n",
    "                   30: [[] for _ in range(max_sample_count)], 40: [[] for _ in range(max_sample_count)],\n",
    "                   60: [[] for _ in range(max_sample_count)], 120:[[] for _ in range(max_sample_count)]}\n",
    "\n",
    "true_label =      {10: [[] for _ in range(max_sample_count)], 20: [[] for _ in range(max_sample_count)],\n",
    "                   30: [[] for _ in range(max_sample_count)], 40: [[] for _ in range(max_sample_count)],\n",
    "                   60: [[] for _ in range(max_sample_count)], 120:[[] for _ in range(max_sample_count)]}\n",
    "\n",
    "for i in range(1, 201):\n",
    "  # print(f'model {i}')\n",
    "  for d in [10, 20, 30, 40, 60, 120]:\n",
    "    \n",
    "    model_path = f'./models/models/model_{i}_{d}d.ckpt'\n",
    "    # create testing dataset\n",
    "\n",
    "    try:\n",
    "      test_set = StockDataset(f'./data/training_data/testing_data_{i}_{d}d.csv')\n",
    "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      for k in range(max_sample_count):\n",
    "        predicted_value[d][k].append(0)\n",
    "        predicted_label[d][k].append(1)\n",
    "        true_label[d][k].append(1)\n",
    "      continue\n",
    "\n",
    "    # create model and load weights from checkpoint\n",
    "    model = Classifier().to(device)\n",
    "    model.load_state_dict(torch.load(model_path), strict=False)\n",
    "\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      days = 0\n",
    "      for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = nn.Softmax(dim=1)(model(inputs))\n",
    "        \n",
    "        test_value, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
    "              \n",
    "\n",
    "        for j in range(len(outputs)):\n",
    "          predicted_value[d][days + j].append(test_value.cpu().numpy()[j])\n",
    "          predicted_label[d][days + j].append(test_pred.cpu().numpy()[j])\n",
    "          true_label[d][days + j].append(labels.cpu().numpy()[j])\n",
    "            \n",
    "        days += len(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tax_rate = 0.0038\n",
    "annualized_return = {10: [], 20: [], 30: [], 40: [], 60: [], 120: []}\n",
    "roi_long_record = {10: [], 20: [], 30: [], 40: [], 60: [], 120: []}\n",
    "roi_short_record = {10: [], 20: [], 30: [], 40: [], 60: [], 120: []}\n",
    "\n",
    "for d in [10, 20, 30, 40, 60, 120]:\n",
    "    for day_shift in range(0, 85, 21):\n",
    "    # for day_shift in [0, 246, 494, 738, 982, 1228, 1475, 1717]:\n",
    "        compound_interest = 1\n",
    "        for day in range(0, 240, d):\n",
    "            predicted_value_n = predicted_value[d][day + day_shift]\n",
    "            predicted_label_n = predicted_label[d][day + day_shift]\n",
    "            true_label_n = true_label[d][day + day_shift]\n",
    "\n",
    "            sorted_value = []\n",
    "            for i, value in enumerate(predicted_value_n):\n",
    "                sorted_value.append((value, i))\n",
    "            sorted_value = sorted(sorted_value)\n",
    "            \n",
    "            # roi = 0\n",
    "            roi_long = 0\n",
    "            roi_short = 0\n",
    "\n",
    "            # stock_selected_count = 0\n",
    "            long_count = 0\n",
    "            short_count = 0\n",
    "          \n",
    "            long_weight = 0\n",
    "            short_weight = 0\n",
    "\n",
    "            for value, idx in reversed(sorted_value):\n",
    "\n",
    "                if value < 0.50 and long_count >= 5 and short_count >= 5:\n",
    "                    break\n",
    "                # if value < 0.4:\n",
    "                #     break\n",
    "\n",
    "                weight = 1\n",
    "                if predicted_label_n[idx] == 2 and long_count < 10: # long this stock\n",
    "                    long_weight += weight\n",
    "                    if true_label_n[idx] == 2: # hit\n",
    "                        roi_long += weight * (code_para_map[codes[idx]][f'{d}d'][0] - 1)\n",
    "                    elif true_label_n[idx] == 0: # lose\n",
    "                        roi_long -= weight * (code_para_map[codes[idx]][f'{d}d'][0] - 1)\n",
    "\n",
    "                    elif true_label_n[idx] == 1: # stable\n",
    "                        close_ser = pd.read_csv(f'./data/training_data/close_df_{idx + 1}.csv')\n",
    "                        close_ser = close_ser[close_ser['date'] >= '2021-01-01']['close']\n",
    "                        roi_long += weight * ((close_ser.iloc[day + day_shift + d] / close_ser.iloc[day + day_shift]) - 1)\n",
    "                      \n",
    "                    long_count += 1\n",
    "\n",
    "                elif predicted_label_n[idx] == 0 and short_count < 10: # short this stock\n",
    "                    short_weight += weight\n",
    "                    if true_label_n[idx] == 0: # hit\n",
    "                        roi_short += weight * (1 - code_para_map[codes[idx]][f'{d}d'][1]) * (10 / 9)\n",
    "                    elif true_label_n[idx] == 2: #lose\n",
    "                        roi_short -= weight * (1 - code_para_map[codes[idx]][f'{d}d'][1]) * (10 / 9)\n",
    "\n",
    "                    elif true_label_n[idx] == 1: # stable\n",
    "                        close_ser = pd.read_csv(f'./data/training_data/close_df_{idx + 1}.csv')\n",
    "                        close_ser = close_ser[close_ser['date'] >= '2021-01-01']['close']\n",
    "                        roi_short += weight * (1 - (close_ser.iloc[day + day_shift + d] / close_ser.iloc[day + day_shift])) * (10 / 9)\n",
    "                    \n",
    "                    short_count += 1\n",
    "\n",
    "                if long_count >= 10 and short_count >= 10:\n",
    "                    break\n",
    "\n",
    "            if long_count < 5 or short_count < 5:\n",
    "                print(long_count, short_count)\n",
    "\n",
    "            roi_long /= long_weight\n",
    "            roi_short /= short_weight\n",
    "            roi_long_record[d].append(roi_long * compound_interest)\n",
    "            roi_short_record[d].append(roi_short * compound_interest)\n",
    "            roi_total = ((roi_long + roi_short) / 2) - tax_rate # roi in the day\n",
    "            compound_interest *= 1 + roi_total\n",
    "\n",
    "        annualized_return[d].append(compound_interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 days :       3.49 ( 2.44 / 1.05 )\n",
      "20 days :       14.01 ( 7.29 / 6.72 )\n",
      "30 days :       9.79 ( 7.23 / 2.56 )\n",
      "40 days :       7.84 ( 6.23 / 1.61 )\n",
      "60 days :       13.14 ( 8.79 / 4.35 )\n",
      "120 days :       9.41 ( 8.6 / 0.81 )\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "for d in [10, 20, 30, 40, 60, 120]:\n",
    "    avg_long = mean(roi_long_record[d])\n",
    "    avg_short = mean(roi_short_record[d])\n",
    "    avg_total = avg_long + avg_short\n",
    "    irr = round((mean(annualized_return[d]) - 1) * 100, 2)\n",
    "    \n",
    "    print(f'{d} days :      ',\n",
    "            irr,\n",
    "            '(',\n",
    "            round((avg_long / avg_total) * irr, 2),\n",
    "            '/',\n",
    "            round((avg_short / avg_total) * irr, 2),\n",
    "            ')'\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2021/01/04\\n~\\n2021/12/27',\n",
      "'2021/02/02\\n~\\n2022/01/26',\n",
      "'2021/03/15\\n~\\n2022/03/08',\n",
      "'2021/04/15\\n~\\n2022/04/08',\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0fa8196d7e16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# for day_shift in [0, 246, 494, 738, 982, 1228, 1475, 1717]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\''\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclose_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mday_shift\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mr'\\n~\\n'\u001b[0m  \u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclose_ser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mday_shift\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m240\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m'\\','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1498\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m     \u001b[1;31m# -------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "close_ser = pd.read_csv(f'./data/training_data/testing_data_{idx + 1}_10d.csv')\n",
    "for day_shift in range(0, 85, 21):\n",
    "# for day_shift in [0, 246, 494, 738, 982, 1228, 1475, 1717]:\n",
    "\n",
    "    print('\\'' +str(close_ser.iloc[day_shift]['date']).replace('-', '/') + r'\\n~\\n'  +str(close_ser.iloc[day_shift + 240]['date']).replace('-', '/')+ '\\',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2021/01/04\\n~\\n2021/12/27',\n",
      "'2021/02/02\\n~\\n2022/01/26',\n",
      "'2021/03/15\\n~\\n2022/03/08',\n",
      "'2021/04/15\\n~\\n2022/04/08',\n",
      "'2021/05/17\\n~\\n2022/05/10',\n",
      "8.53\n",
      "TAIEX [0.21117324284006922, 0.12146852325976143, 0.035442692098689665, 0.01216919164266228, 0.04609971805190738]\n",
      "(10, [1.0570572398377371, 0.9974422130577079, 1.065454903739989, 0.9992981664051598, 1.0551640266516396])\n",
      "(20, [1.1988570706333703, 1.1343592874005335, 1.1515236363831993, 1.0549768156323467, 1.160767206428383])\n",
      "(30, [1.1413067592629824, 1.1197400982133567, 1.097825795678601, 1.112203474788802, 1.0185549235000113])\n",
      "(40, [1.086208984122144, 1.1409768238747682, 1.0681374139139086, 1.0634734207027088, 1.0332075553120017])\n",
      "(60, [1.1410721043324918, 1.2222056938623795, 1.130110852771257, 1.1030150278448911, 1.0605275181531835])\n",
      "(120, [1.133396514210903, 1.0604466503986272, 1.1586188937876458, 1.1250182713146615, 0.9928922231452734])\n"
     ]
    }
   ],
   "source": [
    "# Baseline TAIEX\n",
    "import pandas as pd\n",
    "\n",
    "baseline = []\n",
    "df = pd.read_csv(f'./data/indicator_data/taiex.csv')\n",
    "df = df[df['date'] >= '2021-01-01']\n",
    "for day_shift in range(0, 85, 21):\n",
    "# for day_shift in [0, 246, 494, 738, 982, 1228, 1475, 1717]:\n",
    "    baseline.append((df['TAIEX'].iloc[day_shift + 240] / df['TAIEX'].iloc[day_shift]) - 1)\n",
    "    print('\\'' +str(df.iloc[day_shift]['date']).replace('-', '/') + r'\\n~\\n'  +str(df.iloc[day_shift + 240]['date']).replace('-', '/')+ '\\',')\n",
    "print(round(mean(baseline) * 100, 2))\n",
    "\n",
    "print('TAIEX' ,baseline)\n",
    "for irr in annualized_return.items():\n",
    "    print(irr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.95\n",
      "baseline [0.26537324284006925, 0.17566852325976143, 0.08964269209868966, 0.06636919164266228, 0.10029971805190738]\n"
     ]
    }
   ],
   "source": [
    "# 2013~2020\n",
    "# average_dividend_yield = [3.61, 4.12, 4.33, 5.67, 3.78, 5.62, 6.7, 5.61]\n",
    "# 2021\n",
    "average_dividend_yield = [5.42]\n",
    "# 2009~2011\n",
    "# average_dividend_yield = [10.3, 0, 8.54]\n",
    "for i in range(len(baseline)):\n",
    "    baseline[i] += average_dividend_yield[0] * 0.01\n",
    "print(round(mean(baseline) * 100, 2))\n",
    "\n",
    "print('baseline' ,baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "N = len(baseline)\n",
    "# N = 5\n",
    "ind = np.arange(N) \n",
    "width = 0.1\n",
    "  \n",
    "vals_taiex = [x * 100 for x in baseline]\n",
    "bar1 = plt.bar(ind, vals_taiex, width, color = 'r')\n",
    "  \n",
    "vals_10 = [(x - 1) * 100 for x in annualized_return[10]]\n",
    "bar2 = plt.bar(ind+width, vals_10, width, color='g')\n",
    "  \n",
    "vals_20 = [(x - 1) * 100 for x in annualized_return[20]]\n",
    "bar3 = plt.bar(ind+width*2, vals_20, width, color = 'm')\n",
    "\n",
    "vals_30 = [(x - 1) * 100 for x in annualized_return[30]]\n",
    "bar4 = plt.bar(ind+width*3, vals_30, width, color = 'darkgray')\n",
    "\n",
    "vals_40 = [(x - 1) * 100 for x in annualized_return[40]]\n",
    "bar5 = plt.bar(ind+width*4, vals_40, width, color = 'darkorange')\n",
    "\n",
    "vals_60 = [(x - 1) * 100 for x in annualized_return[60]]\n",
    "bar6 = plt.bar(ind+width*5, vals_60, width, color = 'y')\n",
    "\n",
    "vals_120 = [(x - 1) * 100 for x in annualized_return[120]]\n",
    "bar7 = plt.bar(ind+width*6, vals_120, width, color = 'c')\n",
    "  \n",
    "plt.xlabel(\"Date Period\")\n",
    "# plt.xlabel(\"Year\")\n",
    "plt.ylabel('Rate of Return (%)')\n",
    "plt.title(\"Annualized Return\")\n",
    "plt.ylim(-1, 27)\n",
    "  \n",
    "plt.xticks(ind+width*2,['2021/01/04\\n~\\n2021/12/27',\n",
    "'2021/02/02\\n~\\n2022/01/26',\n",
    "'2021/03/15\\n~\\n2022/03/08',\n",
    "'2021/04/15\\n~\\n2022/04/08',\n",
    "'2021/05/17\\n~\\n2022/05/10'\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend((bar1, bar2, bar3, bar4, bar5, bar6, bar7),\n",
    "           ('baseline', '10d', '20d', '30d', '40d', '60d', '120d'),\n",
    "           ncol = 2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 13.95\n",
      "10d 3.49\n",
      "20d 14.01\n",
      "30d 9.79\n",
      "40d 7.84\n",
      "60d 13.14\n",
      "120d 9.41\n"
     ]
    }
   ],
   "source": [
    "# Average Annualized Return\n",
    "print('baseline', round(mean(baseline) * 100, 2))\n",
    "print('10d', round((mean(annualized_return[10]) - 1) * 100, 2))\n",
    "print('20d', round((mean(annualized_return[20]) - 1) * 100, 2))\n",
    "print('30d', round((mean(annualized_return[30]) - 1) * 100, 2))\n",
    "print('40d', round((mean(annualized_return[40]) - 1) * 100, 2))\n",
    "print('60d', round((mean(annualized_return[60]) - 1) * 100, 2))\n",
    "print('120d', round((mean(annualized_return[120]) - 1) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82d4c6f819cf47785f735f902f00da8643513d08dab4f4c7470bccf934b8d2d6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
